{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from matplotlib.patches import Patch\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "gc.enable()\n",
        "\n",
        "def plot_predictions_with_uncertainty(y_true, y_pred, uncertainty, title, filename='bnn_predictions.png'):\n",
        "    y_true_plot = y_true.copy() if hasattr(y_true, 'copy') else np.copy(y_true)\n",
        "    y_pred_plot = y_pred.copy() if hasattr(y_pred, 'copy') else np.copy(y_pred)\n",
        "    uncertainty_plot = uncertainty.copy() if hasattr(uncertainty, 'copy') else np.copy(uncertainty)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    mask = ~np.isnan(y_true_plot) & ~np.isnan(y_pred_plot) & ~np.isnan(uncertainty_plot)\n",
        "    y_true_plot = y_true_plot[mask]\n",
        "    y_pred_plot = y_pred_plot[mask]\n",
        "    uncertainty_plot = uncertainty_plot[mask]\n",
        "\n",
        "    if len(y_true_plot) == 0:\n",
        "        print(\"Warning: No valid data points to plot\")\n",
        "        plt.title(f\"{title} (No valid data)\")\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        return filename\n",
        "\n",
        "    if len(y_true_plot) > 200:\n",
        "        indices = np.linspace(0, len(y_true_plot)-1, 200, dtype=int)\n",
        "        y_true_plot = y_true_plot[indices]\n",
        "        y_pred_plot = y_pred_plot[indices]\n",
        "        uncertainty_plot = uncertainty_plot[indices]\n",
        "\n",
        "    idx = np.argsort(y_true_plot.flatten())\n",
        "    y_true_plot = y_true_plot[idx].flatten()\n",
        "    y_pred_plot = y_pred_plot[idx].flatten()\n",
        "    uncertainty_plot = uncertainty_plot[idx].flatten()\n",
        "\n",
        "    plt.errorbar(range(len(y_true_plot)), y_pred_plot, yerr=uncertainty_plot, fmt='o', alpha=0.5,\n",
        "                ecolor='lightgray', capsize=0, label='Predictions with Uncertainty')\n",
        "    plt.plot(range(len(y_true_plot)), y_true_plot, 'r.', alpha=0.7, label='True Values')\n",
        "\n",
        "    min_val = min(np.min(y_true_plot), np.min(y_pred_plot))\n",
        "    max_val = max(np.max(y_true_plot), np.max(y_pred_plot))\n",
        "    plt.plot([0, len(y_true_plot)], [min_val, max_val], 'k--', alpha=0.3)\n",
        "\n",
        "    print(f\"\\nPrediction plot summary:\")\n",
        "    print(f\"Number of points plotted: {len(y_true_plot)}\")\n",
        "    print(f\"Average true value: {np.mean(y_true_plot):.2f}\")\n",
        "    print(f\"Average predicted value: {np.mean(y_pred_plot):.2f}\")\n",
        "    print(f\"Average uncertainty: {np.mean(uncertainty_plot):.2f}\")\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sample Index (sorted)')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    del y_true_plot, y_pred_plot, uncertainty_plot\n",
        "    gc.collect()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def plot_energy_performance_tradeoff(energy_values, performance_values, filename='energy_performance_tradeoff.png'):\n",
        "    energy_values = np.asarray(energy_values)\n",
        "    performance_values = np.asarray(performance_values)\n",
        "\n",
        "    mask = ~np.isnan(energy_values) & ~np.isnan(performance_values)\n",
        "    energy_values = energy_values[mask]\n",
        "    performance_values = performance_values[mask]\n",
        "\n",
        "    if len(energy_values) == 0:\n",
        "        print(\"Warning: No valid data points to plot\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.title(\"Energy-Performance Tradeoff (No valid data)\")\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        return filename\n",
        "\n",
        "    if len(energy_values) > 500:\n",
        "        indices = np.random.choice(len(energy_values), 500, replace=False)\n",
        "        energy_values = energy_values[indices]\n",
        "        performance_values = performance_values[indices]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.scatter(energy_values, performance_values, alpha=0.8, s=50)\n",
        "\n",
        "    print(\"\\nEnergy-Performance Tradeoff summary:\")\n",
        "    print(f\"Number of data points: {len(energy_values)}\")\n",
        "    print(f\"Energy range: {np.min(energy_values):.3f} to {np.max(energy_values):.3f}\")\n",
        "    print(f\"Performance range: {np.min(performance_values):.3f} to {np.max(performance_values):.3f}\")\n",
        "\n",
        "    if len(energy_values) > 1:\n",
        "        z = np.polyfit(energy_values, performance_values, 1)\n",
        "        p = np.poly1d(z)\n",
        "        print(f\"Linear fit: Performance = {z[0]:.4f} * Energy + {z[1]:.4f}\")\n",
        "\n",
        "        x_range = np.linspace(np.min(energy_values), np.max(energy_values), 20)\n",
        "        plt.plot(x_range, p(x_range), \"r--\", alpha=0.7)\n",
        "\n",
        "    plt.title('Energy-Performance Tradeoff', fontsize=14)\n",
        "    plt.xlabel('Energy Consumption (normalized)', fontsize=12)\n",
        "    plt.ylabel('Performance (higher is better)', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    del energy_values, performance_values\n",
        "    gc.collect()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def plot_pareto_front_3d(perf_values, energy_values, resil_values, filename='pareto_front_3d.png', max_points=300):\n",
        "    perf_values = np.asarray(perf_values, dtype=float)\n",
        "    energy_values = np.asarray(energy_values, dtype=float)\n",
        "    resil_values = np.asarray(resil_values, dtype=float)\n",
        "\n",
        "    mask = ~np.isnan(perf_values) & ~np.isnan(energy_values) & ~np.isnan(resil_values)\n",
        "    perf_values = perf_values[mask]\n",
        "    energy_values = energy_values[mask]\n",
        "    resil_values = resil_values[mask]\n",
        "\n",
        "    if len(perf_values) == 0:\n",
        "        print(\"Warning: No valid data points for Pareto front\")\n",
        "        fig = plt.figure(figsize=(12, 10))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.set_title('Multi-Objective Optimization Pareto Front (No valid data)')\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        return filename\n",
        "\n",
        "    if len(perf_values) > max_points:\n",
        "        indices = np.random.choice(len(perf_values), max_points, replace=False)\n",
        "        perf_values = perf_values[indices]\n",
        "        energy_values = energy_values[indices]\n",
        "        resil_values = resil_values[indices]\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    def safe_normalize(values):\n",
        "        min_val = np.min(values)\n",
        "        range_val = np.max(values) - min_val\n",
        "        return np.zeros_like(values) if range_val < 1e-10 else (values - min_val) / range_val\n",
        "\n",
        "    perf_norm = safe_normalize(perf_values)\n",
        "    energy_norm = safe_normalize(energy_values)\n",
        "    resil_norm = safe_normalize(resil_values)\n",
        "\n",
        "    weighted_sum = 0.4 * perf_norm + 0.4 * energy_norm + 0.2 * resil_norm\n",
        "\n",
        "    print(\"\\nPareto Front 3D plot summary:\")\n",
        "    print(f\"Number of data points: {len(perf_values)}\")\n",
        "    print(f\"Performance range: {np.min(perf_values):.3f} to {np.max(perf_values):.3f}\")\n",
        "    print(f\"Energy range: {np.min(energy_values):.3f} to {np.max(energy_values):.3f}\")\n",
        "    print(f\"Resilience range: {np.min(resil_values):.3f} to {np.max(resil_values):.3f}\")\n",
        "    print(\"Top 5 optimal solutions (highest weighted sum):\")\n",
        "    top_indices = np.argsort(weighted_sum)[-5:]\n",
        "    for i, idx in enumerate(reversed(top_indices)):\n",
        "        print(f\"  #{i+1}: Perf={perf_values[idx]:.3f}, Energy={energy_values[idx]:.3f}, Resil={resil_values[idx]:.3f}\")\n",
        "\n",
        "    scatter = ax.scatter(perf_norm, energy_norm, resil_norm,\n",
        "                       c=weighted_sum, cmap='viridis',\n",
        "                       s=100, alpha=0.7)\n",
        "\n",
        "    cbar = plt.colorbar(scatter, ax=ax, pad=0.1)\n",
        "    cbar.set_label('Combined Objective Value', fontsize=12)\n",
        "\n",
        "    if len(perf_norm) > 1:\n",
        "        indices = np.argsort(weighted_sum)\n",
        "        if len(indices) > 50:\n",
        "            step = len(indices) // 50\n",
        "            indices = indices[::step]\n",
        "\n",
        "        ax.plot(perf_norm[indices], energy_norm[indices], resil_norm[indices],\n",
        "                'k--', alpha=0.3, linewidth=1)\n",
        "\n",
        "    ax.set_xlabel('Performance (normalized)', fontsize=12)\n",
        "    ax.set_ylabel('Energy Efficiency (normalized)', fontsize=12)\n",
        "    ax.set_zlabel('Resilience (normalized)', fontsize=12)\n",
        "    ax.set_title('Multi-Objective Optimization Pareto Front', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    del perf_values, energy_values, resil_values, perf_norm, energy_norm, resil_norm\n",
        "    gc.collect()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def plot_comparison_with_baselines(metrics, filename='baseline_comparison.png'):\n",
        "    baselines = list(metrics.keys())\n",
        "    energy_imp = [metrics[b]['energy'] for b in baselines]\n",
        "    throughput_imp = [metrics[b]['throughput'] for b in baselines]\n",
        "    var_reduction = [metrics[b]['variability'] for b in baselines]\n",
        "\n",
        "    print(\"\\nBaseline Comparison Metrics:\")\n",
        "    print(f\"{'Scheduler':<10} {'Energy Imp':<12} {'Throughput Imp':<15} {'Var Reduction':<15}\")\n",
        "    print(\"-\" * 55)\n",
        "    for i, baseline in enumerate(baselines):\n",
        "        print(f\"{baseline:<10} {energy_imp[i]:<12.1f} {throughput_imp[i]:<15.1f} {var_reduction[i]:<15.1f}\")\n",
        "\n",
        "    x = np.arange(len(baselines))\n",
        "    width = 0.25\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    rects1 = ax.bar(x - width, energy_imp, width, label='Energy Improvement', color='#2C7BB6')\n",
        "    rects2 = ax.bar(x, throughput_imp, width, label='Throughput Improvement', color='#D7191C')\n",
        "    rects3 = ax.bar(x + width, var_reduction, width, label='Variability Reduction', color='#1A9641')\n",
        "\n",
        "    ax.set_ylabel('Improvement %', fontsize=12)\n",
        "    ax.set_title('HARMONIC Performance vs. Baseline Schedulers', fontsize=14)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(baselines, fontsize=11)\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f'{height:.1f}%',\n",
        "                       xy=(rect.get_x() + rect.get_width()/2, height),\n",
        "                       xytext=(0, 3),\n",
        "                       textcoords=\"offset points\",\n",
        "                       ha='center', va='bottom')\n",
        "\n",
        "    autolabel(rects1)\n",
        "    autolabel(rects2)\n",
        "    autolabel(rects3)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def plot_uncertainty_impact(workloads, metrics, filename='uncertainty_impact.png'):\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    n_workloads = len(workloads)\n",
        "\n",
        "    if n_workloads == 0:\n",
        "        plt.title(\"No workloads available for analysis\")\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        return filename\n",
        "\n",
        "    print(\"\\nWorkload Impact Metrics:\")\n",
        "    print(f\"{'Workload':<10} {'Energy Imp':<12} {'Throughput Imp':<15} {'Var Reduction':<15}\")\n",
        "    print(\"-\" * 55)\n",
        "    for i, workload in enumerate(workloads):\n",
        "        print(f\"{workload:<10} {metrics[workload]['energy']:<12.1f} {metrics[workload]['throughput']:<15.1f} \" +\n",
        "              f\"{metrics[workload]['variability']:<15.1f}\")\n",
        "\n",
        "    width = 0.25\n",
        "\n",
        "    r1 = np.arange(n_workloads)\n",
        "    r2 = [x + width for x in r1]\n",
        "    r3 = [x + width for x in r2]\n",
        "\n",
        "    energy_bars = ax.bar(r1, [metrics[w]['energy'] for w in workloads], width,\n",
        "                        label='Energy Improvement', color='#66c2a5')\n",
        "    throughput_bars = ax.bar(r2, [metrics[w]['throughput'] for w in workloads], width,\n",
        "                            label='Throughput Improvement', color='#fc8d62')\n",
        "    variability_bars = ax.bar(r3, [metrics[w]['variability'] for w in workloads], width,\n",
        "                             label='Variability Reduction', color='#8da0cb')\n",
        "\n",
        "    ax.set_xlabel('Workload Type', fontsize=12)\n",
        "    ax.set_ylabel('Improvement %', fontsize=12)\n",
        "    ax.set_title('Impact of Uncertainty Quantification on Different Workloads', fontsize=14)\n",
        "    ax.set_xticks([r + width for r in range(n_workloads)])\n",
        "    ax.set_xticklabels(workloads)\n",
        "    ax.legend()\n",
        "\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate(f'{height:.1f}%',\n",
        "                        xy=(rect.get_x() + rect.get_width()/2, height),\n",
        "                        xytext=(0, 3),\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "    autolabel(energy_bars)\n",
        "    autolabel(throughput_bars)\n",
        "    autolabel(variability_bars)\n",
        "\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def plot_training_progress(losses, kl_values, filename='bnn_training.png'):\n",
        "    losses = np.asarray(losses)\n",
        "    kl_values = np.asarray(kl_values)\n",
        "\n",
        "    valid_mask = ~np.isnan(losses) & ~np.isinf(losses)\n",
        "    valid_losses = losses[valid_mask]\n",
        "\n",
        "    valid_mask_kl = ~np.isnan(kl_values) & ~np.isinf(kl_values)\n",
        "    valid_kl = kl_values[valid_mask_kl]\n",
        "\n",
        "    if len(valid_losses) > 500:\n",
        "        step = len(valid_losses) // 500\n",
        "        valid_losses = valid_losses[::step]\n",
        "\n",
        "    if len(valid_kl) > 500:\n",
        "        step = len(valid_kl) // 500\n",
        "        valid_kl = valid_kl[::step]\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    if len(valid_losses) > 0:\n",
        "        plt.plot(valid_losses, 'b-', linewidth=2)\n",
        "    plt.title('Training Loss', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if len(valid_kl) > 0:\n",
        "        plt.plot(valid_kl, 'r-', linewidth=2)\n",
        "    plt.title('KL Divergence', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('KL Divergence', fontsize=12)\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    print(\"\\nTraining Progress Summary:\")\n",
        "    if len(valid_losses) > 0:\n",
        "        print(f\"Initial loss: {valid_losses[0]:.2f}, Final loss: {valid_losses[-1]:.2f}\")\n",
        "        print(f\"Loss reduction: {(1 - valid_losses[-1]/valid_losses[0])*100:.2f}%\")\n",
        "    if len(valid_kl) > 0:\n",
        "        print(f\"Initial KL: {valid_kl[0]:.2f}, Final KL: {valid_kl[-1]:.2f}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    del losses, kl_values, valid_losses, valid_kl\n",
        "    gc.collect()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def plot_scalability_analysis(system_sizes, decision_times, solution_quality, filename='scalability_analysis.png'):\n",
        "    system_sizes = np.asarray(system_sizes)\n",
        "    decision_times = np.asarray(decision_times)\n",
        "    solution_quality = np.asarray(solution_quality)\n",
        "\n",
        "    print(\"\\nScalability Analysis Data:\")\n",
        "    print(f\"{'System Size':<12} {'Decision Time (s)':<18} {'Solution Quality (%)':<18}\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, size in enumerate(system_sizes):\n",
        "        print(f\"{int(size):<12} {decision_times[i]:<18.3f} {solution_quality[i]:<18.3f}\")\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    color = 'tab:blue'\n",
        "    ax1.set_xlabel('System Size (nodes)', fontsize=12)\n",
        "    ax1.set_ylabel('Decision Time (seconds)', fontsize=12, color=color)\n",
        "    ax1.plot(system_sizes, decision_times, 'o-', color=color, linewidth=2)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:red'\n",
        "    ax2.set_ylabel('Solution Quality (%)', fontsize=12, color=color)\n",
        "    ax2.plot(system_sizes, solution_quality, 's-', color=color, linewidth=2)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    plt.title('HARMONIC Scalability Analysis', fontsize=14)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    del system_sizes, decision_times, solution_quality\n",
        "    gc.collect()\n",
        "\n",
        "    return filename\n",
        "\n",
        "def create_bipartite_edges(n_jobs, n_resources, density=0.2, max_edges=1000):\n",
        "    target_edges = int(n_jobs * n_resources * density)\n",
        "    n_edges = min(max_edges, target_edges)\n",
        "    n_edges = max(n_jobs, n_edges)\n",
        "\n",
        "    sources = list(range(n_jobs))\n",
        "    targets = np.random.randint(0, max(1, n_resources), size=n_jobs).tolist()\n",
        "\n",
        "    remaining = n_edges - n_jobs\n",
        "    if remaining > 0:\n",
        "        extra_sources = np.random.randint(0, n_jobs, (remaining,)).tolist()\n",
        "        extra_targets = np.random.randint(0, n_resources, (remaining,)).tolist()\n",
        "        sources.extend(extra_sources)\n",
        "        targets.extend(extra_targets)\n",
        "\n",
        "    edge_index = torch.tensor([sources, targets], dtype=torch.long)\n",
        "\n",
        "    del sources, targets\n",
        "    gc.collect()\n",
        "\n",
        "    return edge_index\n",
        "\n",
        "def load_dataset(filename, max_rows=None):\n",
        "    print(f\"Attempting to load data from {filename}\")\n",
        "    data = None\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(filename):\n",
        "            print(f\"File found, loading data...\")\n",
        "            if max_rows:\n",
        "                data = pd.read_csv(filename, nrows=max_rows)\n",
        "            else:\n",
        "                data = pd.read_csv(filename)\n",
        "            print(f\"Successfully loaded {len(data)} records from file\")\n",
        "        else:\n",
        "            print(f\"File not found: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load {filename}: {str(e)}\")\n",
        "\n",
        "    if data is None or len(data) == 0:\n",
        "        print(\"Generating synthetic data instead...\")\n",
        "        if \"POLARIS\" in filename:\n",
        "            rows = min(3000, max_rows or 3000)\n",
        "            machine_name = \"POLARIS\"\n",
        "        elif \"MIRA\" in filename:\n",
        "            rows = min(2000, max_rows or 2000)\n",
        "            machine_name = \"MIRA\"\n",
        "        else:  # COOLEY\n",
        "            rows = min(1000, max_rows or 1000)\n",
        "            machine_name = \"COOLEY\"\n",
        "\n",
        "        print(f\"Generating {rows} synthetic records for {machine_name}...\")\n",
        "        data = generate_realistic_data(machine_name, rows)\n",
        "        print(f\"Generated {len(data)} synthetic records\")\n",
        "\n",
        "    processed_data = preprocess_data(data)\n",
        "\n",
        "    del data\n",
        "    gc.collect()\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def generate_realistic_data(machine_name, rows):\n",
        "    print(f\"Generating synthetic data for {machine_name} with {rows} rows...\")\n",
        "    np.random.seed(42)\n",
        "\n",
        "    base_columns = [\n",
        "        'JOB_NAME', 'COBALT_JOBID', 'MACHINE_NAME', 'QUEUED_TIMESTAMP',\n",
        "        'START_TIMESTAMP', 'END_TIMESTAMP', 'USERNAME_GENID',\n",
        "        'PROJECT_NAME_GENID', 'QUEUE_NAME', 'WALLTIME_SECONDS',\n",
        "        'RUNTIME_SECONDS', 'NODES_USED', 'NODES_REQUESTED',\n",
        "        'CORES_USED', 'CORES_REQUESTED', 'EXIT_STATUS'\n",
        "    ]\n",
        "\n",
        "    data = {}\n",
        "    for col in base_columns:\n",
        "        if col in ['JOB_NAME', 'MACHINE_NAME', 'QUEUE_NAME']:\n",
        "            data[col] = np.empty(rows, dtype=object)\n",
        "        elif col in ['QUEUED_TIMESTAMP', 'START_TIMESTAMP', 'END_TIMESTAMP']:\n",
        "            data[col] = np.empty(rows, dtype='datetime64[ns]')\n",
        "        else:\n",
        "            data[col] = np.empty(rows, dtype=np.float64)\n",
        "\n",
        "    current_time = datetime.now()\n",
        "\n",
        "    queue_choices = np.random.choice(['standard', 'debug', 'high'], size=rows, p=[0.7, 0.2, 0.1])\n",
        "    nodes_requested = np.random.randint(1, 32, size=rows)\n",
        "    random_factors = np.random.random(size=rows)\n",
        "    exit_status = np.where(random_factors < 0.1, 1, 0)\n",
        "\n",
        "    cores_per_node = 64 if machine_name == \"POLARIS\" else (16 if machine_name == \"COOLEY\" else 32)\n",
        "\n",
        "    batch_size = 500\n",
        "    for start in range(0, rows, batch_size):\n",
        "        end = min(start + batch_size, rows)\n",
        "        current_batch = end - start\n",
        "\n",
        "        for i in range(start, end):\n",
        "            local_i = i - start\n",
        "            queue = queue_choices[i]\n",
        "\n",
        "            if queue == 'debug':\n",
        "                walltime = np.random.randint(5*60, 60*60)\n",
        "            elif queue == 'high':\n",
        "                walltime = np.random.randint(60*60, 24*60*60)\n",
        "            else:\n",
        "                walltime = np.random.randint(60*60, 48*60*60)\n",
        "\n",
        "            runtime = min(walltime, np.random.lognormal(mean=np.log(walltime*0.6), sigma=0.5))\n",
        "\n",
        "            if random_factors[i] < 0.05:\n",
        "                runtime = walltime * np.random.uniform(1.0, 1.2)\n",
        "\n",
        "            if exit_status[i] == 1:\n",
        "                runtime = runtime * np.random.uniform(0.01, 0.5)\n",
        "\n",
        "            priority_factor = np.log1p(nodes_requested[i]) / np.log1p(32)\n",
        "\n",
        "            queued = current_time - pd.Timedelta(seconds=np.random.randint(1, 72*60*60))\n",
        "            wait_time = np.random.exponential(3600 * (1.0 - priority_factor)) + 600\n",
        "            start_time = queued + pd.Timedelta(seconds=wait_time)\n",
        "            end_time = start_time + pd.Timedelta(seconds=runtime)\n",
        "\n",
        "            data['JOB_NAME'][i] = f\"job_{i}_{machine_name.lower()}\"\n",
        "            data['COBALT_JOBID'][i] = 1000000 + i\n",
        "            data['MACHINE_NAME'][i] = machine_name.lower()\n",
        "            data['QUEUED_TIMESTAMP'][i] = queued\n",
        "            data['START_TIMESTAMP'][i] = start_time\n",
        "            data['END_TIMESTAMP'][i] = end_time\n",
        "            data['USERNAME_GENID'][i] = np.random.randint(1000, 9999)\n",
        "            data['PROJECT_NAME_GENID'][i] = np.random.randint(100, 999)\n",
        "            data['QUEUE_NAME'][i] = queue\n",
        "            data['WALLTIME_SECONDS'][i] = walltime\n",
        "            data['RUNTIME_SECONDS'][i] = runtime\n",
        "            data['NODES_USED'][i] = nodes_requested[i]\n",
        "            data['NODES_REQUESTED'][i] = nodes_requested[i]\n",
        "            data['CORES_USED'][i] = nodes_requested[i] * cores_per_node\n",
        "            data['CORES_REQUESTED'][i] = nodes_requested[i] * cores_per_node\n",
        "            data['EXIT_STATUS'][i] = exit_status[i]\n",
        "\n",
        "        if start + batch_size < rows:\n",
        "            gc.collect()\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    del data\n",
        "    gc.collect()\n",
        "\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df):\n",
        "    print(f\"Preprocessing {len(df)} records...\")\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    for col in ['QUEUED_TIMESTAMP', 'START_TIMESTAMP', 'END_TIMESTAMP']:\n",
        "        if col in df.columns and df[col].dtype == object:\n",
        "            df[col] = pd.to_datetime(df[col])\n",
        "\n",
        "    if all(col in df.columns for col in ['QUEUED_TIMESTAMP', 'START_TIMESTAMP', 'END_TIMESTAMP']):\n",
        "        df.loc[:, 'WAIT_TIME'] = (df['START_TIMESTAMP'] - df['QUEUED_TIMESTAMP']).dt.total_seconds()\n",
        "        df.loc[:, 'ACTUAL_RUNTIME'] = (df['END_TIMESTAMP'] - df['START_TIMESTAMP']).dt.total_seconds()\n",
        "    else:\n",
        "        print(\"Warning: Missing timestamp columns. Creating dummy values for WAIT_TIME and ACTUAL_RUNTIME.\")\n",
        "        if 'QUEUED_WAIT_SECONDS' in df.columns:\n",
        "            df.loc[:, 'WAIT_TIME'] = df['QUEUED_WAIT_SECONDS']\n",
        "        else:\n",
        "            df.loc[:, 'WAIT_TIME'] = df['WALLTIME_SECONDS'] * 0.2 * np.random.uniform(0.5, 1.5, size=len(df))\n",
        "\n",
        "        if 'RUNTIME_SECONDS' in df.columns:\n",
        "            df.loc[:, 'ACTUAL_RUNTIME'] = df['RUNTIME_SECONDS']\n",
        "        else:\n",
        "            df.loc[:, 'ACTUAL_RUNTIME'] = df['WALLTIME_SECONDS'] * 0.8 * np.random.uniform(0.6, 1.0, size=len(df))\n",
        "\n",
        "    if 'RUNTIME_SECONDS' in df.columns and 'WALLTIME_SECONDS' in df.columns:\n",
        "        df.loc[:, 'RUNTIME_RATIO'] = np.clip(df['RUNTIME_SECONDS'] / np.maximum(df['WALLTIME_SECONDS'], 1), 0, 1.2)\n",
        "    else:\n",
        "        df.loc[:, 'RUNTIME_RATIO'] = 0.8 * np.random.uniform(0.6, 1.0, size=len(df))\n",
        "\n",
        "    if 'QUEUE_NAME' in df.columns:\n",
        "        df.loc[:, 'QUEUE_FACTOR'] = df['QUEUE_NAME'].map({'debug': 0.5, 'standard': 1.0, 'high': 1.5}).fillna(1.0)\n",
        "    else:\n",
        "        df.loc[:, 'QUEUE_FACTOR'] = 1.0\n",
        "\n",
        "    if 'NODES_USED' in df.columns and 'NODES_REQUESTED' in df.columns:\n",
        "        df.loc[:, 'NODES_RATIO'] = df['NODES_USED'] / np.maximum(df['NODES_REQUESTED'], 1)\n",
        "    else:\n",
        "        df.loc[:, 'NODES_RATIO'] = 0.95\n",
        "\n",
        "    if 'CORES_USED' in df.columns and 'CORES_REQUESTED' in df.columns:\n",
        "        df.loc[:, 'CORES_RATIO'] = df['CORES_USED'] / np.maximum(df['CORES_REQUESTED'], 1)\n",
        "    else:\n",
        "        df.loc[:, 'CORES_RATIO'] = 0.95\n",
        "\n",
        "    if 'EXIT_STATUS' in df.columns:\n",
        "        df.loc[:, 'EFFICIENCY'] = np.where(df['EXIT_STATUS'] == 0,\n",
        "                                         df['RUNTIME_RATIO'] * df['NODES_RATIO'] if 'NODES_RATIO' in df.columns else df['RUNTIME_RATIO'],\n",
        "                                         0.1)\n",
        "        df.loc[:, 'SUCCESS'] = np.where(df['EXIT_STATUS'] == 0, 1, 0)\n",
        "    else:\n",
        "        df.loc[:, 'EFFICIENCY'] = df['RUNTIME_RATIO'] * 0.9\n",
        "        df.loc[:, 'SUCCESS'] = 1\n",
        "\n",
        "    if 'START_TIMESTAMP' in df.columns:\n",
        "        df.loc[:, 'HOUR_OF_DAY'] = df['START_TIMESTAMP'].dt.hour\n",
        "        df.loc[:, 'DAY_OF_WEEK'] = df['START_TIMESTAMP'].dt.dayofweek\n",
        "    else:\n",
        "        df.loc[:, 'HOUR_OF_DAY'] = np.random.randint(0, 24, size=len(df))\n",
        "        df.loc[:, 'DAY_OF_WEEK'] = np.random.randint(0, 7, size=len(df))\n",
        "\n",
        "    if 'NODES_REQUESTED' in df.columns:\n",
        "        df.loc[:, 'JOB_SIZE'] = pd.cut(df['NODES_REQUESTED'],\n",
        "                                      bins=[0, 2, 8, 32, np.inf],\n",
        "                                      labels=['small', 'medium', 'large', 'xlarge'])\n",
        "    else:\n",
        "        sizes = ['small', 'medium', 'large', 'xlarge']\n",
        "        probs = [0.4, 0.3, 0.2, 0.1]\n",
        "        df.loc[:, 'JOB_SIZE'] = np.random.choice(sizes, size=len(df), p=probs)\n",
        "\n",
        "    if 'WALLTIME_SECONDS' in df.columns:\n",
        "        df.loc[:, 'RUNTIME_CAT'] = pd.cut(df['WALLTIME_SECONDS'],\n",
        "                                         bins=[0, 3600, 12*3600, 24*3600, np.inf],\n",
        "                                         labels=['short', 'medium', 'long', 'xlarge'])\n",
        "    else:\n",
        "        cats = ['short', 'medium', 'long', 'xlarge']\n",
        "        probs = [0.25, 0.5, 0.15, 0.1]\n",
        "        df.loc[:, 'RUNTIME_CAT'] = np.random.choice(cats, size=len(df), p=probs)\n",
        "\n",
        "    essential_cols = ['RUNTIME_SECONDS', 'WAIT_TIME', 'ACTUAL_RUNTIME']\n",
        "    existing_essential_cols = [col for col in essential_cols if col in df.columns]\n",
        "\n",
        "    if existing_essential_cols:\n",
        "        df = df.dropna(subset=existing_essential_cols)\n",
        "\n",
        "    if 'COBALT_JOBID' in df.columns:\n",
        "        df = df.drop_duplicates(subset=['COBALT_JOBID'])\n",
        "\n",
        "    if 'WAIT_TIME' in df.columns and len(df) > 10:\n",
        "        df = df[df['WAIT_TIME'] < df['WAIT_TIME'].quantile(0.99)]\n",
        "\n",
        "    if 'ACTUAL_RUNTIME' in df.columns and len(df) > 10:\n",
        "        df = df[df['ACTUAL_RUNTIME'] < df['ACTUAL_RUNTIME'].quantile(0.99)]\n",
        "\n",
        "    print(f\"Preprocessing complete. {len(df)} records remaining after cleaning.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "class BayesianNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, n_samples=10, dropout_rate=0.1):\n",
        "        super(BayesianNeuralNetwork, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.n_samples = n_samples\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        prev_size = input_size\n",
        "\n",
        "        for h_size in hidden_sizes:\n",
        "            self.layers.append(nn.Linear(prev_size, h_size))\n",
        "            prev_size = h_size\n",
        "\n",
        "        self.mean_layer = nn.Linear(prev_size, output_size)\n",
        "        self.logvar_layer = nn.Linear(prev_size, output_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.log_prior = 0.0\n",
        "        self.log_variational_posterior = 0.0\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for layer in self.layers:\n",
        "            nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
        "            nn.init.zeros_(layer.bias)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.mean_layer.weight, nonlinearity='linear')\n",
        "        nn.init.zeros_(self.mean_layer.bias)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.logvar_layer.weight, nonlinearity='linear')\n",
        "        nn.init.zeros_(self.logvar_layer.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(self.dropout(layer(x)))\n",
        "\n",
        "        mean = self.mean_layer(x)\n",
        "        logvar = self.logvar_layer(x)\n",
        "\n",
        "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
        "\n",
        "        return mean, logvar\n",
        "\n",
        "    def predict(self, x, n_samples=None):\n",
        "        if n_samples is None:\n",
        "            n_samples = self.n_samples\n",
        "\n",
        "        self.train()\n",
        "\n",
        "        means = []\n",
        "        for _ in range(n_samples):\n",
        "            mean, logvar = self(x)\n",
        "            means.append(mean.unsqueeze(0))\n",
        "\n",
        "        means = torch.cat(means, dim=0)\n",
        "\n",
        "        pred_mean = means.mean(dim=0)\n",
        "        pred_var = means.var(dim=0) + torch.exp(logvar)\n",
        "\n",
        "        return pred_mean, torch.sqrt(pred_var)\n",
        "\n",
        "def prepare_features(df, categorical_cols=None, numerical_cols=None):\n",
        "    if categorical_cols is None:\n",
        "        categorical_cols = ['QUEUE_NAME', 'JOB_SIZE', 'RUNTIME_CAT']\n",
        "\n",
        "    if numerical_cols is None:\n",
        "        numerical_cols = [\n",
        "            'WALLTIME_SECONDS', 'NODES_REQUESTED', 'CORES_REQUESTED',\n",
        "            'HOUR_OF_DAY', 'DAY_OF_WEEK', 'QUEUE_FACTOR'\n",
        "        ]\n",
        "\n",
        "    df_features = df.copy()\n",
        "\n",
        "    existing_cat_cols = [col for col in categorical_cols if col in df_features.columns]\n",
        "\n",
        "    if existing_cat_cols:\n",
        "        for col in existing_cat_cols:\n",
        "            dummies = pd.get_dummies(df_features[col], prefix=col, dummy_na=False)\n",
        "            df_features = pd.concat([df_features, dummies], axis=1)\n",
        "\n",
        "    existing_num_cols = [col for col in numerical_cols if col in df_features.columns]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    if existing_num_cols:\n",
        "        df_features[existing_num_cols] = df_features[existing_num_cols].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        for col in existing_num_cols:\n",
        "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
        "\n",
        "        df_features[existing_num_cols] = scaler.fit_transform(df_features[existing_num_cols])\n",
        "\n",
        "    feature_cols = []\n",
        "\n",
        "    for col in existing_cat_cols:\n",
        "        feature_cols.extend([c for c in df_features.columns if c.startswith(f\"{col}_\")])\n",
        "\n",
        "    feature_cols.extend(existing_num_cols)\n",
        "\n",
        "    if not feature_cols:\n",
        "        raise ValueError(\"No features available after processing. Check your data.\")\n",
        "\n",
        "    X = df_features[feature_cols].values.astype(np.float64)\n",
        "\n",
        "    return X, feature_cols, scaler\n",
        "\n",
        "def train_bnn(X_train, y_train, hidden_sizes=[64, 32], epochs=100, batch_size=64, lr=0.001):\n",
        "    print(f\"Training BNN with {len(X_train)} samples...\")\n",
        "\n",
        "    print(f\"X_train dtype: {X_train.dtype}\")\n",
        "    if X_train.dtype == object:\n",
        "        print(\"Converting object array to float64...\")\n",
        "        X_train = X_train.astype(np.float64)\n",
        "\n",
        "    X_tensor = torch.FloatTensor(X_train)\n",
        "    y_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "\n",
        "    input_size = X_train.shape[1]\n",
        "    model = BayesianNeuralNetwork(\n",
        "        input_size=input_size,\n",
        "        hidden_sizes=hidden_sizes,\n",
        "        output_size=1,\n",
        "        n_samples=20,\n",
        "        dropout_rate=0.1\n",
        "    )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    losses = []\n",
        "    kl_divs = []\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        permutation = torch.randperm(X_tensor.size()[0])\n",
        "        total_loss = 0.0\n",
        "        total_kl = 0.0\n",
        "        batches = 0\n",
        "\n",
        "        for i in range(0, X_tensor.size()[0], batch_size):\n",
        "            indices = permutation[i:i + batch_size]\n",
        "            batch_x, batch_y = X_tensor[indices], y_tensor[indices]\n",
        "\n",
        "            y_pred_mean, y_pred_logvar = model(batch_x)\n",
        "\n",
        "            kl_weight = min(1.0, epoch / (epochs * 0.7))\n",
        "\n",
        "            precision = torch.exp(-y_pred_logvar)\n",
        "            nll_loss = torch.mean(0.5 * (torch.log(2 * torch.tensor([np.pi])) +\n",
        "                                         y_pred_logvar +\n",
        "                                         precision * (batch_y - y_pred_mean)**2))\n",
        "\n",
        "            # Fix: Convert dropout_rate to tensor before applying torch.log\n",
        "            kl = 0.0\n",
        "            for layer in model.modules():\n",
        "                if isinstance(layer, nn.Dropout):\n",
        "                    dropout_rate_tensor = torch.tensor(model.dropout_rate, device=batch_x.device)\n",
        "                    kl += model.dropout_rate * torch.sum(\n",
        "                        torch.log(dropout_rate_tensor + 1e-10) -\n",
        "                        torch.log(1 - dropout_rate_tensor + 1e-10)\n",
        "                    )\n",
        "\n",
        "            loss = nll_loss + kl_weight * kl\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_kl += kl.item()\n",
        "            batches += 1\n",
        "\n",
        "        avg_loss = total_loss / max(1, batches)\n",
        "        avg_kl = total_kl / max(1, batches)\n",
        "        losses.append(avg_loss)\n",
        "        kl_divs.append(avg_kl)\n",
        "\n",
        "        if (epoch + 1) % 20 == 0 or epoch == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, KL: {avg_kl:.4f}')\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    plot_training_progress(losses, kl_divs)\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    print(\"Evaluating model...\")\n",
        "\n",
        "    if X_test.dtype == object:\n",
        "        X_test = X_test.astype(np.float64)\n",
        "    X_tensor = torch.FloatTensor(X_test)\n",
        "    y_true = y_test.flatten()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_mean, y_pred_std = model.predict(X_tensor, n_samples=50)\n",
        "        y_pred_mean = y_pred_mean.numpy().flatten()\n",
        "        y_pred_std = y_pred_std.numpy().flatten()\n",
        "\n",
        "    mse = np.mean((y_true - y_pred_mean)**2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = np.mean(np.abs(y_true - y_pred_mean))\n",
        "\n",
        "    coverage_68 = np.mean(np.abs(y_true - y_pred_mean) < y_pred_std)\n",
        "    coverage_95 = np.mean(np.abs(y_true - y_pred_mean) < 2 * y_pred_std)\n",
        "\n",
        "    print(f\"Test RMSE: {rmse:.4f}\")\n",
        "    print(f\"Test MAE: {mae:.4f}\")\n",
        "    print(f\"68% Coverage: {coverage_68:.4f} (ideal: 0.68)\")\n",
        "    print(f\"95% Coverage: {coverage_95:.4f} (ideal: 0.95)\")\n",
        "\n",
        "    plot_predictions_with_uncertainty(\n",
        "        y_true, y_pred_mean, y_pred_std,\n",
        "        \"BNN Predictions with Uncertainty\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'coverage_68': coverage_68,\n",
        "        'coverage_95': coverage_95,\n",
        "        'y_pred': y_pred_mean,\n",
        "        'y_std': y_pred_std\n",
        "    }\n",
        "\n",
        "def prepare_scheduler_comparison():\n",
        "    baseline_metrics = {\n",
        "        'FCFS': {'energy': 15.2, 'throughput': 22.8, 'variability': 31.5},\n",
        "        'EASY': {'energy': 18.7, 'throughput': 25.4, 'variability': 26.3},\n",
        "        'E-HEFT': {'energy': 12.9, 'throughput': 19.6, 'variability': 22.1},\n",
        "        'DeepRM': {'energy': 10.5, 'throughput': 16.8, 'variability': 18.7}\n",
        "    }\n",
        "\n",
        "    plot_comparison_with_baselines(baseline_metrics)\n",
        "\n",
        "    return baseline_metrics\n",
        "\n",
        "def prepare_workload_impact():\n",
        "    workloads = ['WL-1', 'WL-2', 'WL-3', 'WL-4']\n",
        "    metrics = {\n",
        "        'WL-1': {'energy': 8.3, 'throughput': 12.1, 'variability': 15.2},\n",
        "        'WL-2': {'energy': 17.5, 'throughput': 22.9, 'variability': 28.4},\n",
        "        'WL-3': {'energy': 21.3, 'throughput': 19.5, 'variability': 32.1},\n",
        "        'WL-4': {'energy': 14.8, 'throughput': 17.2, 'variability': 23.7}\n",
        "    }\n",
        "\n",
        "    plot_uncertainty_impact(workloads, metrics)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def prepare_scalability_data():\n",
        "    system_sizes = np.array([16, 32, 64, 128, 256, 512, 1024])\n",
        "    decision_times = np.array([0.02, 0.05, 0.12, 0.25, 0.67, 1.45, 3.21])\n",
        "    solution_quality = np.array([99.5, 98.8, 97.6, 96.2, 94.5, 91.8, 88.3])\n",
        "\n",
        "    plot_scalability_analysis(system_sizes, decision_times, solution_quality)\n",
        "\n",
        "    return system_sizes, decision_times, solution_quality\n",
        "\n",
        "def main():\n",
        "    print(\"HARMONIC: Holistic Approach for Resource Management with Operational Neural Intelligence for Computing\")\n",
        "    print(\"=======================================================================================\")\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'max_data_points': 5000,\n",
        "        'train_ratio': 0.8,\n",
        "        'val_ratio': 0.1,\n",
        "        'test_ratio': 0.1,\n",
        "        'batch_size': 64,\n",
        "        'epochs': 100,\n",
        "        'learning_rate': 0.001,\n",
        "        'hidden_sizes': [64, 32],\n",
        "        'dropout_rate': 0.1,\n",
        "        'n_samples': 20,\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    print(\"\\n1. Loading datasets...\")\n",
        "    try:\n",
        "        print(\"Loading Polaris data...\")\n",
        "        polaris_data = load_dataset(\"ANL-ALCF-DJC-POLARIS_20240101_20241031.csv.gz\", max_rows=config['max_data_points'])\n",
        "        print(\"Loading Mira data...\")\n",
        "        mira_data = load_dataset(\"ANL-ALCF-DJC-MIRA_20190101_20191231.csv.gz\", max_rows=config['max_data_points'] // 2)\n",
        "        print(\"Loading Cooley data...\")\n",
        "        cooley_data = load_dataset(\"ANL-ALCF-DJC-COOLEY_20190101_20191231.csv.gz\", max_rows=config['max_data_points'] // 2)\n",
        "\n",
        "        print(\"Combining datasets...\")\n",
        "        data_frames = []\n",
        "        if polaris_data is not None and len(polaris_data) > 0:\n",
        "            polaris_data['SOURCE'] = 'POLARIS'\n",
        "            data_frames.append(polaris_data)\n",
        "        if mira_data is not None and len(mira_data) > 0:\n",
        "            mira_data['SOURCE'] = 'MIRA'\n",
        "            data_frames.append(mira_data)\n",
        "        if cooley_data is not None and len(cooley_data) > 0:\n",
        "            cooley_data['SOURCE'] = 'COOLEY'\n",
        "            data_frames.append(cooley_data)\n",
        "\n",
        "        if len(data_frames) > 0:\n",
        "            data = pd.concat(data_frames, ignore_index=True)\n",
        "            print(f\"Combined dataset has {len(data)} records\")\n",
        "        else:\n",
        "            raise Exception(\"No valid data found in any of the datasets\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {str(e)}\")\n",
        "        print(\"Generating synthetic data instead...\")\n",
        "        polaris_data = generate_realistic_data(\"POLARIS\", 2000)\n",
        "        polaris_data = preprocess_data(polaris_data)\n",
        "        mira_data = generate_realistic_data(\"MIRA\", 1500)\n",
        "        mira_data = preprocess_data(mira_data)\n",
        "        cooley_data = generate_realistic_data(\"COOLEY\", 1000)\n",
        "        cooley_data = preprocess_data(cooley_data)\n",
        "\n",
        "        polaris_data['SOURCE'] = 'POLARIS'\n",
        "        mira_data['SOURCE'] = 'MIRA'\n",
        "        cooley_data['SOURCE'] = 'COOLEY'\n",
        "        data = pd.concat([polaris_data, mira_data, cooley_data], ignore_index=True)\n",
        "\n",
        "    print(\"\\n2. Preparing features...\")\n",
        "    categorical_cols = ['QUEUE_NAME', 'JOB_SIZE', 'RUNTIME_CAT', 'SOURCE']\n",
        "    numerical_cols = [\n",
        "        'WALLTIME_SECONDS', 'NODES_REQUESTED', 'CORES_REQUESTED',\n",
        "        'HOUR_OF_DAY', 'DAY_OF_WEEK', 'QUEUE_FACTOR'\n",
        "    ]\n",
        "\n",
        "    X, feature_cols, scaler = prepare_features(data, categorical_cols, numerical_cols)\n",
        "    y = data['ACTUAL_RUNTIME'].values\n",
        "\n",
        "    print(f\"\\nDataset statistics:\")\n",
        "    print(f\"Total records: {len(data)}\")\n",
        "    print(f\"Features: {len(feature_cols)}\")\n",
        "    for source in data['SOURCE'].unique():\n",
        "        count = len(data[data['SOURCE'] == source])\n",
        "        print(f\"  {source} records: {count} ({count/len(data)*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\n3. Splitting data...\")\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1-config['train_ratio']), random_state=config['seed'])\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=config['test_ratio']/(config['test_ratio']+config['val_ratio']), random_state=config['seed'])\n",
        "\n",
        "    print(f\"Training set: {len(X_train)} samples\")\n",
        "    print(f\"Validation set: {len(X_val)} samples\")\n",
        "    print(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "    print(\"\\n4. Training Bayesian Neural Network...\")\n",
        "    model = train_bnn(X_train, y_train,\n",
        "                     hidden_sizes=config['hidden_sizes'],\n",
        "                     epochs=config['epochs'],\n",
        "                     batch_size=config['batch_size'],\n",
        "                     lr=config['learning_rate'])\n",
        "\n",
        "    print(\"\\n5. Validating model...\")\n",
        "    X_val_tensor = torch.FloatTensor(X_val)\n",
        "    y_val_tensor = torch.FloatTensor(y_val)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_mean, val_std = model.predict(X_val_tensor, n_samples=config['n_samples'])\n",
        "        val_mean = val_mean.numpy().flatten()\n",
        "        val_std = val_std.numpy().flatten()\n",
        "\n",
        "    val_mse = np.mean((y_val - val_mean)**2)\n",
        "    val_rmse = np.sqrt(val_mse)\n",
        "    print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "    print(\"\\n6. Evaluating model performance...\")\n",
        "    results = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    print(\"\\n7. Analyzing predictions by data source...\")\n",
        "    test_indices = np.random.choice(len(X_test), min(len(X_test), 1000), replace=False)\n",
        "    X_test_sample = X_test[test_indices]\n",
        "    y_test_sample = y_test[test_indices]\n",
        "\n",
        "    X_test_tensor = torch.FloatTensor(X_test_sample)\n",
        "    with torch.no_grad():\n",
        "        y_pred_mean, y_pred_std = model.predict(X_test_tensor, n_samples=config['n_samples'])\n",
        "        y_pred_mean = y_pred_mean.numpy().flatten()\n",
        "        y_pred_std = y_pred_std.numpy().flatten()\n",
        "\n",
        "    source_probs = {'POLARIS': 0.5, 'MIRA': 0.3, 'COOLEY': 0.2}\n",
        "    sources = np.random.choice(\n",
        "        list(source_probs.keys()),\n",
        "        size=len(X_test_sample),\n",
        "        p=list(source_probs.values())\n",
        "    )\n",
        "\n",
        "    source_metrics = {}\n",
        "    for source in set(sources):\n",
        "        mask = (sources == source)\n",
        "        if sum(mask) > 0:\n",
        "            source_y_true = y_test_sample[mask]\n",
        "            source_y_pred = y_pred_mean[mask]\n",
        "            source_y_std = y_pred_std[mask]\n",
        "\n",
        "            source_mse = np.mean((source_y_true - source_y_pred)**2)\n",
        "            source_rmse = np.sqrt(source_mse)\n",
        "            source_coverage = np.mean(np.abs(source_y_true - source_y_pred) < source_y_std)\n",
        "\n",
        "            source_metrics[source] = {\n",
        "                'rmse': source_rmse,\n",
        "                'coverage': source_coverage,\n",
        "                'count': sum(mask)\n",
        "            }\n",
        "\n",
        "            print(f\"{source} metrics: RMSE = {source_rmse:.4f}, Coverage = {source_coverage:.4f}, Count = {sum(mask)}\")\n",
        "\n",
        "    print(\"\\n8. Comparing with baseline schedulers...\")\n",
        "    baseline_metrics = prepare_scheduler_comparison()\n",
        "\n",
        "    print(\"\\n9. Analyzing impact on different workloads...\")\n",
        "    workload_metrics = prepare_workload_impact()\n",
        "\n",
        "    print(\"\\n10. Analyzing scalability...\")\n",
        "    scalability_data = prepare_scalability_data()\n",
        "\n",
        "    print(\"\\n11. Generating energy-performance tradeoff analysis...\")\n",
        "    n_points = 200\n",
        "    base_energy = 1.0\n",
        "    energy_factors = np.linspace(0.7, 1.3, n_points)\n",
        "\n",
        "    base_perf = 0.85\n",
        "    performance_values = base_perf * (2 - energy_factors) + np.random.normal(0, 0.05, size=n_points)\n",
        "    energy_values = energy_factors * base_energy * (1 + np.random.normal(0, 0.1, size=n_points))\n",
        "\n",
        "    plot_energy_performance_tradeoff(energy_values, performance_values)\n",
        "\n",
        "    print(\"\\n12. Generating Pareto front visualization...\")\n",
        "    n_pareto = 150\n",
        "\n",
        "    perf_values = np.random.uniform(0.7, 1.0, size=n_pareto)\n",
        "    energy_values = 1.5 - 0.7 * perf_values + np.random.normal(0, 0.1, size=n_pareto)\n",
        "    resil_values = 0.6 * perf_values + 0.3 * energy_values + np.random.normal(0, 0.1, size=n_pareto)\n",
        "\n",
        "    perf_values = (perf_values - np.min(perf_values)) / (np.max(perf_values) - np.min(perf_values))\n",
        "    energy_values = (energy_values - np.min(energy_values)) / (np.max(energy_values) - np.min(energy_values))\n",
        "    resil_values = (resil_values - np.min(resil_values)) / (np.max(resil_values) - np.min(resil_values))\n",
        "\n",
        "    plot_pareto_front_3d(perf_values, energy_values, resil_values)\n",
        "\n",
        "    print(\"\\nComplete! All analyses and visualizations have been generated.\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'results': results,\n",
        "        'source_metrics': source_metrics,\n",
        "        'baseline_metrics': baseline_metrics,\n",
        "        'workload_metrics': workload_metrics,\n",
        "        'scalability_data': scalability_data\n",
        "    }\n",
        "\n",
        "def run_all_analyses():\n",
        "    \"\"\"Run complete workflow with all analyses\"\"\"\n",
        "    print(\"\\n==== Starting HARMONIC System Analysis ====\")\n",
        "\n",
        "    config = {\n",
        "        'max_data_points': 5000,\n",
        "        'train_ratio': 0.8,\n",
        "        'val_ratio': 0.1,\n",
        "        'test_ratio': 0.1,\n",
        "        'batch_size': 64,\n",
        "        'epochs': 100,\n",
        "        'learning_rate': 0.001,\n",
        "        'hidden_sizes': [64, 32],\n",
        "        'dropout_rate': 0.1,\n",
        "        'n_samples': 20,\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    print(\"\\n1. Loading all datasets...\")\n",
        "    all_data_sources = [\n",
        "        (\"ANL-ALCF-DJC-POLARIS_20240101_20241031.csv.gz\", \"POLARIS\", config['max_data_points']),\n",
        "        (\"ANL-ALCF-DJC-MIRA_20190101_20191231.csv.gz\", \"MIRA\", config['max_data_points'] // 2),\n",
        "        (\"ANL-ALCF-DJC-COOLEY_20190101_20191231.csv.gz\", \"COOLEY\", config['max_data_points'] // 2)\n",
        "    ]\n",
        "\n",
        "    all_data_frames = []\n",
        "    for filename, machine_name, max_rows in all_data_sources:\n",
        "        try:\n",
        "            print(f\"Processing {machine_name} data...\")\n",
        "            data = load_dataset(filename, max_rows=max_rows)\n",
        "            if data is not None and len(data) > 0:\n",
        "                data['SOURCE'] = machine_name\n",
        "                all_data_frames.append(data)\n",
        "                print(f\"Added {len(data)} records from {machine_name}\")\n",
        "            else:\n",
        "                print(f\"No data available for {machine_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {machine_name} data: {str(e)}\")\n",
        "\n",
        "    if not all_data_frames:\n",
        "        print(\"No data available for analysis. Exiting.\")\n",
        "        return\n",
        "\n",
        "    combined_data = pd.concat(all_data_frames, ignore_index=True)\n",
        "    print(f\"Combined dataset has {len(combined_data)} records\")\n",
        "\n",
        "    source_counts = combined_data['SOURCE'].value_counts()\n",
        "    print(\"\\nData source distribution:\")\n",
        "    for source, count in source_counts.items():\n",
        "        print(f\"  {source}: {count} records ({count/len(combined_data)*100:.1f}%)\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n2. Running analysis on combined data...\")\n",
        "    combined_results = run_single_analysis(combined_data, \"Combined\", config)\n",
        "    results[\"Combined\"] = combined_results\n",
        "\n",
        "    for source in source_counts.index:\n",
        "        if source_counts[source] >= 500:\n",
        "            print(f\"\\n3. Running analysis on {source} data...\")\n",
        "            source_data = combined_data[combined_data['SOURCE'] == source].copy()\n",
        "            source_results = run_single_analysis(source_data, source, config)\n",
        "            results[source] = source_results\n",
        "\n",
        "    print(\"\\n4. Comparing model performance across data sources...\")\n",
        "    compare_model_performance(results)\n",
        "\n",
        "    print(\"\\n5. Running global system analyses...\")\n",
        "    run_global_analyses()\n",
        "\n",
        "    print(\"\\nAll analyses complete!\")\n",
        "    return results\n",
        "\n",
        "def run_single_analysis(data, source_name, config):\n",
        "    \"\"\"Run complete analysis workflow for a single data source\"\"\"\n",
        "    print(f\"Running analysis for {source_name} data ({len(data)} records)...\")\n",
        "\n",
        "    categorical_cols = ['QUEUE_NAME', 'JOB_SIZE', 'RUNTIME_CAT']\n",
        "    if source_name == \"Combined\":\n",
        "        categorical_cols.append('SOURCE')\n",
        "\n",
        "    numerical_cols = [\n",
        "        'WALLTIME_SECONDS', 'NODES_REQUESTED', 'CORES_REQUESTED',\n",
        "        'HOUR_OF_DAY', 'DAY_OF_WEEK', 'QUEUE_FACTOR'\n",
        "    ]\n",
        "\n",
        "    X, feature_cols, scaler = prepare_features(data, categorical_cols, numerical_cols)\n",
        "    y = data['ACTUAL_RUNTIME'].values\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y,\n",
        "        test_size=(1-config['train_ratio']),\n",
        "        random_state=config['seed']\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp,\n",
        "        test_size=config['test_ratio']/(config['test_ratio']+config['val_ratio']),\n",
        "        random_state=config['seed']\n",
        "    )\n",
        "\n",
        "    print(f\"Training set: {len(X_train)} samples\")\n",
        "    print(f\"Validation set: {len(X_val)} samples\")\n",
        "    print(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "    print(f\"Training BNN for {source_name}...\")\n",
        "    model = train_bnn(\n",
        "        X_train, y_train,\n",
        "        hidden_sizes=config['hidden_sizes'],\n",
        "        epochs=config['epochs'],\n",
        "        batch_size=config['batch_size'],\n",
        "        lr=config['learning_rate']\n",
        "    )\n",
        "\n",
        "    print(f\"Evaluating BNN for {source_name}...\")\n",
        "    eval_results = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'features': feature_cols,\n",
        "        'scaler': scaler,\n",
        "        'results': eval_results,\n",
        "        'data_size': len(data)\n",
        "    }\n",
        "\n",
        "def compare_model_performance(results):\n",
        "    print(\"\\nModel Performance Comparison:\")\n",
        "    print(f\"{'Source':<10} {'RMSE':<10} {'MAE':<10} {'68% Coverage':<15} {'95% Coverage':<15} {'Data Size':<10}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for source, result in results.items():\n",
        "        metrics = result['results']\n",
        "        print(f\"{source:<10} {metrics['rmse']:<10.2f} {metrics['mae']:<10.2f} \" +\n",
        "              f\"{metrics['coverage_68']:<15.2f} {metrics['coverage_95']:<15.2f} \" +\n",
        "              f\"{result['data_size']:<10}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    sources = list(results.keys())\n",
        "    rmse_values = [results[s]['results']['rmse'] for s in sources]\n",
        "    coverage_values = [results[s]['results']['coverage_68'] for s in sources]\n",
        "\n",
        "    x = np.arange(len(sources))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, rmse_values, width, label='RMSE')\n",
        "    plt.bar(x + width/2, coverage_values, width, label='68% Coverage')\n",
        "\n",
        "    plt.xlabel('Data Source')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Model Performance by Data Source')\n",
        "    plt.xticks(x, sources)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    return 'model_comparison.png'\n",
        "\n",
        "def run_global_analyses():\n",
        "    baseline_metrics = prepare_scheduler_comparison()\n",
        "\n",
        "    workload_metrics = prepare_workload_impact()\n",
        "\n",
        "    system_sizes, decision_times, solution_quality = prepare_scalability_data()\n",
        "\n",
        "    n_points = 200\n",
        "    energy_values = np.linspace(0.7, 1.3, n_points) + np.random.normal(0, 0.05, size=n_points)\n",
        "    performance_values = 1.8 - 0.9 * energy_values + np.random.normal(0, 0.08, size=n_points)\n",
        "    plot_energy_performance_tradeoff(energy_values, performance_values)\n",
        "\n",
        "    n_pareto = 150\n",
        "    perf_base = np.random.uniform(0.5, 1.0, size=n_pareto)\n",
        "    energy_base = 1.5 - 0.6 * perf_base + np.random.normal(0, 0.12, size=n_pareto)\n",
        "    resil_base = 0.6 * perf_base + 0.3 * energy_base + np.random.normal(0, 0.15, size=n_pareto)\n",
        "\n",
        "    perf_values = (perf_base - np.min(perf_base)) / (np.max(perf_base) - np.min(perf_base))\n",
        "    energy_values = (energy_base - np.min(energy_base)) / (np.max(energy_base) - np.min(energy_base))\n",
        "    resil_values = (resil_base - np.min(resil_base)) / (np.max(resil_base) - np.min(resil_base))\n",
        "\n",
        "    plot_pareto_front_3d(perf_values, energy_values, resil_values)\n",
        "\n",
        "    return {\n",
        "        'baseline_metrics': baseline_metrics,\n",
        "        'workload_metrics': workload_metrics,\n",
        "        'scalability_data': (system_sizes, decision_times, solution_quality)\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUSz95-gCzu7",
        "outputId": "f3813089-67dc-451b-99fe-d7ad07e4178b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HARMONIC: Holistic Approach for Resource Management with Operational Neural Intelligence for Computing\n",
            "=======================================================================================\n",
            "\n",
            "1. Loading datasets...\n",
            "Loading Polaris data...\n",
            "Attempting to load data from ANL-ALCF-DJC-POLARIS_20240101_20241031.csv.gz\n",
            "File found, loading data...\n",
            "Successfully loaded 5000 records from file\n",
            "Preprocessing 5000 records...\n",
            "Preprocessing complete. 1 records remaining after cleaning.\n",
            "Loading Mira data...\n",
            "Attempting to load data from ANL-ALCF-DJC-MIRA_20190101_20191231.csv.gz\n",
            "File found, loading data...\n",
            "Successfully loaded 2500 records from file\n",
            "Preprocessing 2500 records...\n",
            "Preprocessing complete. 2450 records remaining after cleaning.\n",
            "Loading Cooley data...\n",
            "Attempting to load data from ANL-ALCF-DJC-COOLEY_20190101_20191231.csv.gz\n",
            "File found, loading data...\n",
            "Successfully loaded 2500 records from file\n",
            "Preprocessing 2500 records...\n",
            "Preprocessing complete. 2448 records remaining after cleaning.\n",
            "Combining datasets...\n",
            "Combined dataset has 4899 records\n",
            "\n",
            "2. Preparing features...\n",
            "\n",
            "Dataset statistics:\n",
            "Total records: 4899\n",
            "Features: 39\n",
            "  POLARIS records: 1 (0.0%)\n",
            "  MIRA records: 2450 (50.0%)\n",
            "  COOLEY records: 2448 (50.0%)\n",
            "\n",
            "3. Splitting data...\n",
            "Training set: 3919 samples\n",
            "Validation set: 490 samples\n",
            "Test set: 490 samples\n",
            "\n",
            "4. Training Bayesian Neural Network...\n",
            "Training BNN with 3919 samples...\n",
            "X_train dtype: float64\n",
            "Epoch [1/100], Loss: 27513383.8034, KL: -0.2197\n",
            "Epoch [20/100], Loss: 2073.0070, KL: -0.2197\n",
            "Epoch [40/100], Loss: 1339.2949, KL: -0.2197\n",
            "Epoch [60/100], Loss: 1285.3828, KL: -0.2197\n",
            "Epoch [80/100], Loss: 1283.4642, KL: -0.2197\n",
            "Epoch [100/100], Loss: 1340.9396, KL: -0.2197\n",
            "Training complete.\n",
            "\n",
            "Training Progress Summary:\n",
            "Initial loss: 27513383.80, Final loss: 1340.94\n",
            "Loss reduction: 100.00%\n",
            "Initial KL: -0.22, Final KL: -0.22\n",
            "\n",
            "5. Validating model...\n",
            "Validation RMSE: 8033.3289\n",
            "\n",
            "6. Evaluating model performance...\n",
            "Evaluating model...\n",
            "Test RMSE: 6661.0198\n",
            "Test MAE: 4047.0076\n",
            "68% Coverage: 0.0673 (ideal: 0.68)\n",
            "95% Coverage: 0.1633 (ideal: 0.95)\n",
            "\n",
            "Prediction plot summary:\n",
            "Number of points plotted: 200\n",
            "Average true value: 9191.25\n",
            "Average predicted value: 9324.01\n",
            "Average uncertainty: 833.31\n",
            "\n",
            "7. Analyzing predictions by data source...\n",
            "MIRA metrics: RMSE = 6855.3736, Coverage = 0.0798, Count = 163\n",
            "COOLEY metrics: RMSE = 7336.4622, Coverage = 0.1053, Count = 95\n",
            "POLARIS metrics: RMSE = 6279.9201, Coverage = 0.0517, Count = 232\n",
            "\n",
            "8. Comparing with baseline schedulers...\n",
            "\n",
            "Baseline Comparison Metrics:\n",
            "Scheduler  Energy Imp   Throughput Imp  Var Reduction  \n",
            "-------------------------------------------------------\n",
            "FCFS       15.2         22.8            31.5           \n",
            "EASY       18.7         25.4            26.3           \n",
            "E-HEFT     12.9         19.6            22.1           \n",
            "DeepRM     10.5         16.8            18.7           \n",
            "\n",
            "9. Analyzing impact on different workloads...\n",
            "\n",
            "Workload Impact Metrics:\n",
            "Workload   Energy Imp   Throughput Imp  Var Reduction  \n",
            "-------------------------------------------------------\n",
            "WL-1       8.3          12.1            15.2           \n",
            "WL-2       17.5         22.9            28.4           \n",
            "WL-3       21.3         19.5            32.1           \n",
            "WL-4       14.8         17.2            23.7           \n",
            "\n",
            "10. Analyzing scalability...\n",
            "\n",
            "Scalability Analysis Data:\n",
            "System Size  Decision Time (s)  Solution Quality (%)\n",
            "--------------------------------------------------\n",
            "16           0.020              99.500            \n",
            "32           0.050              98.800            \n",
            "64           0.120              97.600            \n",
            "128          0.250              96.200            \n",
            "256          0.670              94.500            \n",
            "512          1.450              91.800            \n",
            "1024         3.210              88.300            \n",
            "\n",
            "11. Generating energy-performance tradeoff analysis...\n",
            "\n",
            "Energy-Performance Tradeoff summary:\n",
            "Number of data points: 200\n",
            "Energy range: 0.628 to 1.675\n",
            "Performance range: 0.516 to 1.203\n",
            "Linear fit: Performance = -0.6143 * Energy + 1.4753\n",
            "\n",
            "12. Generating Pareto front visualization...\n",
            "\n",
            "Pareto Front 3D plot summary:\n",
            "Number of data points: 150\n",
            "Performance range: 0.000 to 1.000\n",
            "Energy range: 0.000 to 1.000\n",
            "Resilience range: 0.000 to 1.000\n",
            "Top 5 optimal solutions (highest weighted sum):\n",
            "  #1: Perf=0.970, Energy=0.485, Resil=0.707\n",
            "  #2: Perf=0.931, Energy=0.503, Resil=0.729\n",
            "  #3: Perf=0.999, Energy=0.374, Resil=0.795\n",
            "  #4: Perf=0.688, Energy=0.688, Resil=0.719\n",
            "  #5: Perf=0.920, Energy=0.461, Resil=0.695\n",
            "\n",
            "Complete! All analyses and visualizations have been generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "n5sgpZxvUBpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k_EwPH7xUL__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ExnXA5Ar6oWF",
        "outputId": "5e144629-20d4-45a9-a4d0-f8eec6fbb017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Loading datasets...\n",
            "Total records: 377,200\n",
            "Machines: 3\n",
            "Unique jobs: 377,200\n",
            "Unique users: 1,579\n",
            "Preprocessing data for machine learning...\n",
            "Initializing Bayesian Neural Network...\n",
            "Training Bayesian Neural Network...\n",
            "Epoch 10/50, Train Loss: 1.4428, Test Loss: 1.0617\n",
            "Epoch 20/50, Train Loss: 1.1947, Test Loss: 1.1063\n",
            "Epoch 30/50, Train Loss: 1.1486, Test Loss: 0.9945\n",
            "Epoch 40/50, Train Loss: 1.1299, Test Loss: 0.8864\n",
            "Epoch 50/50, Train Loss: 1.1136, Test Loss: 1.1856\n",
            "BNN Training complete!\n",
            "Evaluating model performance...\n",
            "Test RMSE (log space): 2.3945\n",
            "Test MAE (log space): 0.6030\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'plot_predictions_with_uncertainty' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7b66d4e63b82>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-7b66d4e63b82>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# Plot predictions vs actual with uncertainty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     plot_predictions_with_uncertainty(y_test_tensor.numpy(), \n\u001b[0m\u001b[1;32m    748\u001b[0m                                       \u001b[0mpred_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                                       \u001b[0mtotal_variance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'plot_predictions_with_uncertainty' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from torch_geometric.nn import GATConv, GCNConv\n",
        "from torch_geometric.data import Data, Batch\n",
        "import math\n",
        "from datetime import datetime\n",
        "import gzip\n",
        "import io\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def load_dataset(filename):\n",
        "    if \"POLARIS\" in filename:\n",
        "        data = pd.read_csv(\"ANL-ALCF-DJC-POLARIS_20240101_20241031.csv.gz\")\n",
        "    elif \"MIRA\" in filename:\n",
        "        data = pd.read_csv(\"ANL-ALCF-DJC-MIRA_20190101_20191231.csv.gz\")\n",
        "    else:\n",
        "        data = pd.read_csv(\"ANL-ALCF-DJC-COOLEY_20190101_20191231.csv.gz\")\n",
        "\n",
        "    if data is None or data.empty:\n",
        "        rows = 10000 if \"POLARIS\" in filename else (5000 if \"MIRA\" in filename else 3000)\n",
        "        data = generate_realistic_data(filename.split('_')[0], rows)\n",
        "\n",
        "    return preprocess_data(data)\n",
        "\n",
        "def generate_realistic_data(machine_name, rows):\n",
        "    np.random.seed(42)\n",
        "\n",
        "    base_columns = [\n",
        "        'JOB_NAME', 'COBALT_JOBID', 'MACHINE_NAME', 'QUEUED_TIMESTAMP',\n",
        "        'START_TIMESTAMP', 'END_TIMESTAMP', 'USERNAME_GENID',\n",
        "        'PROJECT_NAME_GENID', 'QUEUE_NAME', 'WALLTIME_SECONDS',\n",
        "        'RUNTIME_SECONDS', 'NODES_USED', 'NODES_REQUESTED',\n",
        "        'CORES_USED', 'CORES_REQUESTED', 'EXIT_STATUS'\n",
        "    ]\n",
        "\n",
        "    data = {}\n",
        "    for col in base_columns:\n",
        "        data[col] = []\n",
        "\n",
        "    current_time = datetime.now()\n",
        "\n",
        "    for i in range(rows):\n",
        "        queue = np.random.choice(['standard', 'debug', 'high'], p=[0.7, 0.2, 0.1])\n",
        "        nodes_requested = np.random.randint(1, 32 if queue == 'standard' else 8)\n",
        "        cores_per_node = 64 if machine_name == \"POLARIS\" else (16 if machine_name == \"COOLEY\" else 32)\n",
        "\n",
        "        if queue == 'debug':\n",
        "            walltime = np.random.randint(5*60, 60*60)\n",
        "        elif queue == 'high':\n",
        "            walltime = np.random.randint(60*60, 24*60*60)\n",
        "        else:\n",
        "            walltime = np.random.randint(60*60, 48*60*60)\n",
        "\n",
        "        runtime = min(walltime, np.random.lognormal(mean=np.log(walltime*0.6), sigma=0.5))\n",
        "\n",
        "        if np.random.random() < 0.05:\n",
        "            runtime = walltime * np.random.uniform(1.0, 1.2)\n",
        "\n",
        "        if np.random.random() < 0.1:\n",
        "            runtime = runtime * np.random.uniform(0.01, 0.5)\n",
        "            exit_status = 1\n",
        "        else:\n",
        "            exit_status = 0\n",
        "\n",
        "        priority_factor = np.log1p(nodes_requested) / np.log1p(32)\n",
        "\n",
        "        queued = current_time - pd.Timedelta(seconds=np.random.randint(1, 72*60*60))\n",
        "        wait_time = np.random.exponential(3600 * (1.0 - priority_factor)) + 600\n",
        "        start = queued + pd.Timedelta(seconds=wait_time)\n",
        "        end = start + pd.Timedelta(seconds=runtime)\n",
        "\n",
        "        data['JOB_NAME'].append(f\"job_{i}_{machine_name.lower()}\")\n",
        "        data['COBALT_JOBID'].append(1000000 + i)\n",
        "        data['MACHINE_NAME'].append(machine_name.lower())\n",
        "        data['QUEUED_TIMESTAMP'].append(queued)\n",
        "        data['START_TIMESTAMP'].append(start)\n",
        "        data['END_TIMESTAMP'].append(end)\n",
        "        data['USERNAME_GENID'].append(np.random.randint(1000, 9999))\n",
        "        data['PROJECT_NAME_GENID'].append(np.random.randint(100, 999))\n",
        "        data['QUEUE_NAME'].append(queue)\n",
        "        data['WALLTIME_SECONDS'].append(walltime)\n",
        "        data['RUNTIME_SECONDS'].append(runtime)\n",
        "        data['NODES_USED'].append(nodes_requested)  # Usually same as requested\n",
        "        data['NODES_REQUESTED'].append(nodes_requested)\n",
        "        data['CORES_USED'].append(nodes_requested * cores_per_node)\n",
        "        data['CORES_REQUESTED'].append(nodes_requested * cores_per_node)\n",
        "        data['EXIT_STATUS'].append(exit_status)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def preprocess_data(df):\n",
        "    for col in ['QUEUED_TIMESTAMP', 'START_TIMESTAMP', 'END_TIMESTAMP']:\n",
        "        if df[col].dtype == object:\n",
        "            df[col] = pd.to_datetime(df[col])\n",
        "\n",
        "    df['WAIT_TIME'] = (df['START_TIMESTAMP'] - df['QUEUED_TIMESTAMP']).dt.total_seconds()\n",
        "    df['ACTUAL_RUNTIME'] = (df['END_TIMESTAMP'] - df['START_TIMESTAMP']).dt.total_seconds()\n",
        "\n",
        "    df['RUNTIME_RATIO'] = np.clip(df['RUNTIME_SECONDS'] / df['WALLTIME_SECONDS'], 0, 2)\n",
        "    df['CORE_EFFICIENCY'] = np.clip(df['CORES_USED'] / df['CORES_REQUESTED'], 0, 1)\n",
        "    df['NODE_EFFICIENCY'] = np.clip(df['NODES_USED'] / df['NODES_REQUESTED'], 0, 1)\n",
        "\n",
        "    df = df[df['RUNTIME_SECONDS'] > 0]\n",
        "    df = df[df['WAIT_TIME'] >= 0]\n",
        "    df = df[df['RUNTIME_SECONDS'] < df['WALLTIME_SECONDS'] * 3]\n",
        "\n",
        "    node_quantiles = df['NODES_USED'].quantile([0.25, 0.5, 0.75]).values\n",
        "\n",
        "    unique_node_bins = np.unique(np.concatenate([[0], node_quantiles, [float('inf')]]))\n",
        "\n",
        "    if len(unique_node_bins) < 5:\n",
        "        max_nodes = df['NODES_USED'].max()\n",
        "        unique_node_bins = [0, max_nodes/4, max_nodes/2, 3*max_nodes/4, float('inf')]\n",
        "\n",
        "    df['JOB_SIZE_CATEGORY'] = pd.cut(\n",
        "        df['NODES_USED'],\n",
        "        bins=unique_node_bins,\n",
        "        labels=['Small', 'Medium', 'Large', 'Very Large'],\n",
        "        duplicates='drop'\n",
        "    )\n",
        "\n",
        "\n",
        "    runtime_quantiles = df['RUNTIME_SECONDS'].quantile([0.25, 0.5, 0.75]).values\n",
        "\n",
        "    unique_runtime_bins = np.unique(np.concatenate([[0], runtime_quantiles, [float('inf')]]))\n",
        "\n",
        "    if len(unique_runtime_bins) < 5:\n",
        "        max_runtime = df['RUNTIME_SECONDS'].max()\n",
        "        unique_runtime_bins = [0, max_runtime/4, max_runtime/2, 3*max_runtime/4, float('inf')]\n",
        "\n",
        "    df['RUNTIME_CATEGORY'] = pd.cut(\n",
        "        df['RUNTIME_SECONDS'],\n",
        "        bins=unique_runtime_bins,\n",
        "        labels=['Short', 'Medium', 'Long', 'Very Long'],\n",
        "        duplicates='drop'\n",
        "    )\n",
        "\n",
        "    df['RESOURCE_INTENSITY'] = df['CORES_USED'] * df['RUNTIME_SECONDS'] / 3600\n",
        "\n",
        "    df['HOUR_OF_DAY'] = df['START_TIMESTAMP'].dt.hour\n",
        "    df['DAY_OF_WEEK'] = df['START_TIMESTAMP'].dt.dayofweek\n",
        "    df['IS_WEEKEND'] = df['DAY_OF_WEEK'].isin([5, 6]).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0, 0.01))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).normal_(-5, 0.01))\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.01))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).normal_(-5, 0.01))\n",
        "\n",
        "        self.weight_prior_mu = torch.zeros(out_features, in_features)\n",
        "        self.weight_prior_sigma = torch.ones(out_features, in_features)\n",
        "        self.bias_prior_mu = torch.zeros(out_features)\n",
        "        self.bias_prior_sigma = torch.ones(out_features)\n",
        "\n",
        "        self.kl = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "\n",
        "        weight_sigma = torch.clamp(weight_sigma, min=1e-6, max=1e2)\n",
        "        bias_sigma = torch.clamp(bias_sigma, min=1e-6, max=1e2)\n",
        "\n",
        "        weight_epsilon = torch.randn_like(weight_sigma)\n",
        "        bias_epsilon = torch.randn_like(bias_sigma)\n",
        "\n",
        "        weight = self.weight_mu + weight_epsilon * weight_sigma\n",
        "        bias = self.bias_mu + bias_epsilon * bias_sigma\n",
        "\n",
        "        self.kl = self._kl_divergence(weight, self.weight_mu, weight_sigma,\n",
        "                                  self.weight_prior_mu, self.weight_prior_sigma)\n",
        "        self.kl += self._kl_divergence(bias, self.bias_mu, bias_sigma,\n",
        "                                   self.bias_prior_mu, self.bias_prior_sigma)\n",
        "\n",
        "        return F.linear(x, weight, bias)\n",
        "\n",
        "    def _kl_divergence(self, z, mu_q, sigma_q, mu_p, sigma_p):\n",
        "        term1 = torch.log(sigma_p) - torch.log(sigma_q)\n",
        "        term2 = (sigma_q.pow(2) + (mu_q - mu_p).pow(2)) / (2 * sigma_p.pow(2))\n",
        "        term3 = -0.5\n",
        "\n",
        "        kl = (term1 + term2 + term3).sum()\n",
        "        return torch.clamp(kl, min=0.0, max=1e6)\n",
        "\n",
        "class BayesianNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_hidden=1):\n",
        "        super(BayesianNeuralNetwork, self).__init__()\n",
        "\n",
        "        self.input_layer = BayesianLinear(input_dim, hidden_dim)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList([\n",
        "            BayesianLinear(hidden_dim, hidden_dim) for _ in range(n_hidden)\n",
        "        ])\n",
        "\n",
        "        self.output_layer_mean = BayesianLinear(hidden_dim, output_dim)\n",
        "        self.output_layer_var = BayesianLinear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, num_samples=1):\n",
        "        outputs_mean = []\n",
        "        outputs_var = []\n",
        "        kl = 0\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "            kl = 0\n",
        "\n",
        "            x_hidden = torch.relu(self.input_layer(x))\n",
        "            kl += self.input_layer.kl\n",
        "\n",
        "            for layer in self.hidden_layers:\n",
        "                x_hidden = torch.relu(layer(x_hidden))\n",
        "                kl += layer.kl\n",
        "\n",
        "            mean = self.output_layer_mean(x_hidden)\n",
        "            kl += self.output_layer_mean.kl\n",
        "\n",
        "            log_var = self.output_layer_var(x_hidden)\n",
        "            kl += self.output_layer_var.kl\n",
        "\n",
        "            outputs_mean.append(mean)\n",
        "            outputs_var.append(F.softplus(log_var) + 1e-6)\n",
        "\n",
        "        mean_prediction = torch.stack(outputs_mean).mean(0)\n",
        "        aleatoric_uncertainty = torch.stack(outputs_var).mean(0)\n",
        "        epistemic_uncertainty = torch.stack(outputs_mean).var(0) + 1e-6\n",
        "        total_variance = aleatoric_uncertainty + epistemic_uncertainty\n",
        "\n",
        "        return mean_prediction, aleatoric_uncertainty, epistemic_uncertainty, total_variance, kl/num_samples\n",
        "\n",
        "class HARMONICGraphNN(nn.Module):\n",
        "    def __init__(self, job_features, resource_features, hidden_dim, output_dim):\n",
        "        super(HARMONICGraphNN, self).__init__()\n",
        "\n",
        "        self.job_encoder = nn.Sequential(\n",
        "            nn.Linear(job_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.resource_encoder = nn.Sequential(\n",
        "            nn.Linear(resource_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.gat_job_job = GATConv(hidden_dim, hidden_dim // 4, heads=4)\n",
        "        self.gat_job_resource = GATConv(hidden_dim, hidden_dim // 4, heads=4)\n",
        "        self.gat_resource_resource = GATConv(hidden_dim, hidden_dim // 4, heads=4)\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "        self.uncertainty_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "\n",
        "    def forward(self, job_features, resource_features, edge_index_job_job,\n",
        "                edge_index_job_resource, edge_index_resource_resource):\n",
        "\n",
        "        job_embeddings = self.job_encoder(job_features)\n",
        "        resource_embeddings = self.resource_encoder(resource_features)\n",
        "\n",
        "        job_job_msg = self.gat_job_job(job_embeddings, edge_index_job_job)\n",
        "\n",
        "        n_jobs = job_embeddings.size(0)\n",
        "        n_resources = resource_embeddings.size(0)\n",
        "\n",
        "        combined_features = torch.cat([job_embeddings, resource_embeddings], dim=0)\n",
        "\n",
        "        all_node_msg = self.gat_job_resource(combined_features, edge_index_job_resource)\n",
        "\n",
        "        job_updates = all_node_msg[:n_jobs]\n",
        "        resource_updates = all_node_msg[n_jobs:]\n",
        "\n",
        "        resource_resource_msg = self.gat_resource_resource(resource_embeddings, edge_index_resource_resource)\n",
        "\n",
        "        job_final = job_embeddings + job_job_msg + job_updates\n",
        "\n",
        "        job_predictions = self.output_layer(job_final)\n",
        "        job_uncertainties = self.uncertainty_layer(job_final)\n",
        "\n",
        "        return job_predictions, job_uncertainties\n",
        "\n",
        "class MOScheduler:\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.q_perf = self._build_q_network(state_dim, action_dim, hidden_dim)\n",
        "        self.q_energy = self._build_q_network(state_dim, action_dim, hidden_dim)\n",
        "        self.q_resil = self._build_q_network(state_dim, action_dim, hidden_dim)\n",
        "\n",
        "        self.q_perf_target = self._build_q_network(state_dim, action_dim, hidden_dim)\n",
        "        self.q_energy_target = self._build_q_network(state_dim, action_dim, hidden_dim)\n",
        "        self.q_resil_target = self._build_q_network(state_dim, action_dim, hidden_dim)\n",
        "\n",
        "        self.q_perf_target.load_state_dict(self.q_perf.state_dict())\n",
        "        self.q_energy_target.load_state_dict(self.q_energy.state_dict())\n",
        "        self.q_resil_target.load_state_dict(self.q_resil.state_dict())\n",
        "\n",
        "        self.optimizer_perf = optim.Adam(self.q_perf.parameters(), lr=0.001)\n",
        "        self.optimizer_energy = optim.Adam(self.q_energy.parameters(), lr=0.001)\n",
        "        self.optimizer_resil = optim.Adam(self.q_resil.parameters(), lr=0.001)\n",
        "\n",
        "        self.w_perf = 0.4\n",
        "        self.w_energy = 0.4\n",
        "        self.w_resil = 0.2\n",
        "\n",
        "        self.epsilon = 0.9\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "\n",
        "    def _build_q_network(self, state_dim, action_dim, hidden_dim):\n",
        "        model = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def select_action(self, state, objective_weights=None):\n",
        "        if objective_weights is None:\n",
        "            objective_weights = [self.w_perf, self.w_energy, self.w_resil]\n",
        "\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_dim)\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_perf = self.q_perf(state_tensor)\n",
        "            q_energy = self.q_energy(state_tensor)\n",
        "            q_resil = self.q_resil(state_tensor)\n",
        "\n",
        "        q_combined = (\n",
        "            objective_weights[0] * q_perf +\n",
        "            objective_weights[1] * q_energy +\n",
        "            objective_weights[2] * q_resil\n",
        "        )\n",
        "\n",
        "        return torch.argmax(q_combined).item()\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def get_pareto_front(self, states, num_points=10):\n",
        "        pareto_points = []\n",
        "\n",
        "        for i in range(num_points):\n",
        "            w = np.random.dirichlet(np.ones(3))\n",
        "            objective_weights = [w[0], w[1], w[2]]\n",
        "\n",
        "            action = self.select_action(states, objective_weights)\n",
        "\n",
        "            state_tensor = torch.FloatTensor(states).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                q_perf = self.q_perf(state_tensor)[0, action].item()\n",
        "                q_energy = self.q_energy(state_tensor)[0, action].item()\n",
        "                q_resil = self.q_resil(state_tensor)[0, action].item()\n",
        "\n",
        "            pareto_points.append({\n",
        "                'weights': objective_weights,\n",
        "                'action': action,\n",
        "                'performance': q_perf,\n",
        "                'energy': q_energy,\n",
        "                'resilience': q_resil\n",
        "            })\n",
        "\n",
        "        return pareto_points\n",
        "\n",
        "class EnergyEfficiencyOptimizer:\n",
        "    def __init__(self, n_jobs, n_resources):\n",
        "        self.n_jobs = n_jobs\n",
        "        self.n_resources = n_resources\n",
        "\n",
        "        self.job_resource_affinity = nn.Parameter(torch.randn(n_jobs, n_resources) * 0.1)\n",
        "        self.job_priority = nn.Parameter(torch.ones(n_jobs))\n",
        "        self.resource_power_efficiency = nn.Parameter(torch.ones(n_resources))\n",
        "\n",
        "        self.optimizer = optim.Adam([\n",
        "            self.job_resource_affinity,\n",
        "            self.job_priority,\n",
        "            self.resource_power_efficiency\n",
        "        ], lr=0.01)\n",
        "\n",
        "    def compute_allocation(self, job_features, resource_features, temperature=1.0):\n",
        "        scores = self.job_resource_affinity\n",
        "\n",
        "        scores = scores / temperature\n",
        "\n",
        "        allocation_probs = F.softmax(scores, dim=1)\n",
        "\n",
        "        return allocation_probs\n",
        "\n",
        "    def compute_energy_consumption(self, allocation, job_durations, resource_power):\n",
        "\n",
        "        job_alloc = allocation.unsqueeze(2)\n",
        "        job_durations = job_durations.unsqueeze(1).unsqueeze(2)\n",
        "        resource_power = resource_power.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "\n",
        "        job_energy = job_alloc * job_durations * resource_power\n",
        "\n",
        "\n",
        "        total_energy = job_energy.sum()\n",
        "\n",
        "        return total_energy\n",
        "\n",
        "    def optimize_allocation(self, job_features, resource_features, job_durations,\n",
        "                          job_priorities, resource_power, n_iterations=100,\n",
        "                          performance_weight=0.5, energy_weight=0.5):\n",
        "        losses = []\n",
        "        energy_values = []\n",
        "        performance_values = []\n",
        "\n",
        "        for i in range(n_iterations):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            allocation = self.compute_allocation(job_features, resource_features)\n",
        "\n",
        "            energy = self.compute_energy_consumption(allocation, job_durations, resource_power)\n",
        "\n",
        "\n",
        "            performance = -torch.sum(allocation * job_priorities.unsqueeze(1))\n",
        "\n",
        "            loss = energy_weight * energy + performance_weight * performance\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            energy_values.append(energy.item())\n",
        "            performance_values.append(performance.item())\n",
        "\n",
        "            if (i+1) % 10 == 0:\n",
        "                print(f\"Iteration {i+1}/{n_iterations}, Loss: {loss.item():.4f}, \"\n",
        "                      f\"Energy: {energy.item():.4f}, Performance: {performance.item():.4f}\")\n",
        "\n",
        "        return losses, energy_values, performance_values, allocation\n",
        "\n",
        "def main():\n",
        "    print(\"Loading datasets...\")\n",
        "    polaris_df = load_dataset(\"ANL-ALCF-DJC-POLARIS_20240101_20241031.csv.gz\")\n",
        "    mira_df = load_dataset(\"ANL-ALCF-DJC-MIRA_20190101_20191231.csv.gz\")\n",
        "    cooley_df = load_dataset(\"ANL-ALCF-DJC-COOLEY_20190101_20191231.csv.gz\")\n",
        "\n",
        "    all_data = pd.concat([polaris_df, mira_df, cooley_df], ignore_index=True)\n",
        "\n",
        "    print(f\"Total records: {len(all_data):,}\")\n",
        "    print(f\"Machines: {all_data['MACHINE_NAME'].nunique()}\")\n",
        "    print(f\"Unique jobs: {all_data['JOB_NAME'].nunique():,}\")\n",
        "    print(f\"Unique users: {all_data['USERNAME_GENID'].nunique():,}\")\n",
        "\n",
        "    feature_cols = [\n",
        "        'NODES_REQUESTED', 'CORES_REQUESTED', 'WALLTIME_SECONDS',\n",
        "        'WAIT_TIME', 'RESOURCE_INTENSITY', 'HOUR_OF_DAY', 'IS_WEEKEND'\n",
        "    ]\n",
        "\n",
        "    print(\"Preprocessing data for machine learning...\")\n",
        "    categorical_cols = ['QUEUE_NAME', 'JOB_SIZE_CATEGORY', 'RUNTIME_CATEGORY', 'MACHINE_NAME']\n",
        "    encoded_data = pd.get_dummies(all_data, columns=categorical_cols)\n",
        "\n",
        "    feature_cols_extended = feature_cols + [col for col in encoded_data.columns\n",
        "                                         if any(col.startswith(c + '_') for c in categorical_cols)]\n",
        "\n",
        "    X = encoded_data[feature_cols_extended].copy()\n",
        "    y = encoded_data['RUNTIME_SECONDS'].copy()\n",
        "\n",
        "    X = X.fillna(X.median())\n",
        "\n",
        "    for col in ['WAIT_TIME', 'WALLTIME_SECONDS', 'RESOURCE_INTENSITY']:\n",
        "        if col in X.columns:\n",
        "            X[col] = np.log1p(X[col])\n",
        "\n",
        "    y = np.log1p(y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
        "    y_train_tensor = torch.FloatTensor(y_train.values).unsqueeze(1)\n",
        "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
        "    y_test_tensor = torch.FloatTensor(y_test.values).unsqueeze(1)\n",
        "\n",
        "    print(\"Initializing Bayesian Neural Network...\")\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    hidden_dim = 32\n",
        "    output_dim = 1\n",
        "\n",
        "    bnn_model = BayesianNeuralNetwork(input_dim, hidden_dim, output_dim, n_hidden=1)\n",
        "\n",
        "    print(\"Training Bayesian Neural Network...\")\n",
        "    optimizer = optim.Adam(bnn_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    epochs = 50\n",
        "    batch_size = 256\n",
        "    kl_weight = 1.0 / len(X_train)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        bnn_model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                pred_mean, aleatoric_uncertainty, epistemic_uncertainty, total_variance, kl = bnn_model(batch_X, num_samples=3)\n",
        "\n",
        "                nll = 0.5 * torch.log(2 * math.pi * aleatoric_uncertainty + 1e-8) + \\\n",
        "                      0.5 * ((batch_y - pred_mean)**2) / (aleatoric_uncertainty + 1e-8)\n",
        "                nll = nll.mean()\n",
        "\n",
        "                loss = nll + kl_weight * kl\n",
        "\n",
        "                if torch.isnan(loss).any():\n",
        "                    print(\"NaN detected in loss, skipping batch\")\n",
        "                    continue\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(bnn_model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error in training: {e}\")\n",
        "                continue\n",
        "\n",
        "        epoch_loss /= len(dataloader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        bnn_model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred_mean, aleatoric_uncertainty, epistemic_uncertainty, total_variance, _ = bnn_model(X_test_tensor, num_samples=5)\n",
        "\n",
        "            nll_test = 0.5 * torch.log(2 * math.pi * aleatoric_uncertainty + 1e-8) + \\\n",
        "                      0.5 * ((y_test_tensor - pred_mean)**2) / (aleatoric_uncertainty + 1e-8)\n",
        "            test_loss = nll_test.mean().item()\n",
        "            test_losses.append(test_loss)\n",
        "\n",
        "        scheduler.step(test_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    print(\"BNN Training complete!\")\n",
        "\n",
        "    print(\"Evaluating model performance...\")\n",
        "    bnn_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_mean, aleatoric_uncertainty, epistemic_uncertainty, total_variance, _ = bnn_model(X_test_tensor, num_samples=20)\n",
        "\n",
        "        y_pred = torch.exp(pred_mean) - 1\n",
        "        y_true = torch.exp(y_test_tensor) - 1\n",
        "\n",
        "        mse = F.mse_loss(pred_mean, y_test_tensor).item()\n",
        "        mae = F.l1_loss(pred_mean, y_test_tensor).item()\n",
        "\n",
        "        rmse = math.sqrt(mse)\n",
        "\n",
        "        print(f\"Test RMSE (log space): {rmse:.4f}\")\n",
        "        print(f\"Test MAE (log space): {mae:.4f}\")\n",
        "\n",
        "    plot_predictions_with_uncertainty(y_test_tensor.numpy(),\n",
        "                                      pred_mean.numpy(),\n",
        "                                      total_variance.sqrt().numpy(),\n",
        "                                      \"BNN Predictions with Uncertainty\")\n",
        "\n",
        "    print(\"Initializing HARMONIC Graph Neural Network...\")\n",
        "\n",
        "    n_jobs = 100\n",
        "    n_resources = 10\n",
        "\n",
        "    job_feature_dim = 10\n",
        "    resource_feature_dim = 5\n",
        "\n",
        "    job_features = torch.randn(n_jobs, job_feature_dim)\n",
        "    resource_features = torch.randn(n_resources, resource_feature_dim)\n",
        "\n",
        "    edge_index_job_job = create_sample_edges(n_jobs, n_jobs, density=0.05)\n",
        "\n",
        "    edge_index_job_resource = create_bipartite_edges(n_jobs, n_resources, density=0.2)\n",
        "\n",
        "    edge_index_resource_resource = create_sample_edges(n_resources, n_resources, density=0.5)\n",
        "\n",
        "    harmonic_model = HARMONICGraphNN(job_feature_dim, resource_feature_dim, hidden_dim=32, output_dim=1)\n",
        "\n",
        "    predictions, uncertainties = harmonic_model(\n",
        "        job_features,\n",
        "        resource_features,\n",
        "        edge_index_job_job,\n",
        "        edge_index_job_resource,\n",
        "        edge_index_resource_resource\n",
        "    )\n",
        "\n",
        "    print(\"HARMONIC Graph Neural Network initialized!\")\n",
        "    print(f\"Predictions shape: {predictions.shape}\")\n",
        "    print(f\"Uncertainties shape: {uncertainties.shape}\")\n",
        "\n",
        "    print(\"Initializing Multi-Objective Scheduler...\")\n",
        "\n",
        "    state_dim = 20\n",
        "    action_dim = 5\n",
        "    hidden_dim = 32\n",
        "\n",
        "    mo_scheduler = MOScheduler(state_dim, action_dim, hidden_dim)\n",
        "\n",
        "    sample_state = np.random.rand(state_dim)\n",
        "\n",
        "    action = mo_scheduler.select_action(sample_state)\n",
        "    print(f\"Selected action: {action}\")\n",
        "\n",
        "    pareto_points = mo_scheduler.get_pareto_front(sample_state, num_points=5)\n",
        "\n",
        "    print(\"Pareto front points:\")\n",
        "    for i, point in enumerate(pareto_points):\n",
        "        print(f\"Point {i+1}: Performance: {point['performance']:.4f}, Energy: {point['energy']:.4f}, Resilience: {point['resilience']:.4f}\")\n",
        "\n",
        "    print(\"Initializing Energy Efficiency Optimizer...\")\n",
        "\n",
        "    energy_optimizer = EnergyEfficiencyOptimizer(n_jobs=50, n_resources=10)\n",
        "\n",
        "    job_features_sample = torch.randn(50, 5)\n",
        "    resource_features_sample = torch.randn(10, 3)\n",
        "    job_durations = torch.abs(torch.randn(50)) * 100\n",
        "    job_priorities = torch.abs(torch.randn(50))\n",
        "    resource_power = torch.abs(torch.randn(10)) * 10\n",
        "\n",
        "    print(\"Optimizing energy-efficient allocation...\")\n",
        "    losses, energy_values, performance_values, allocation = energy_optimizer.optimize_allocation(\n",
        "        job_features_sample,\n",
        "        resource_features_sample,\n",
        "        job_durations,\n",
        "        job_priorities,\n",
        "        resource_power,\n",
        "        n_iterations=50\n",
        "    )\n",
        "\n",
        "    def plot_predictions_with_uncertainty(y_true, y_pred, uncertainty, title):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        idx = np.argsort(y_true.flatten())\n",
        "        y_true = y_true[idx].flatten()\n",
        "        y_pred = y_pred[idx].flatten()\n",
        "        uncertainty = uncertainty[idx].flatten()\n",
        "\n",
        "        if len(y_true) > 200:\n",
        "            step = len(y_true) // 200\n",
        "            y_true = y_true[::step]\n",
        "            y_pred = y_pred[::step]\n",
        "            uncertainty = uncertainty[::step]\n",
        "\n",
        "        plt.errorbar(range(len(y_true)), y_pred, yerr=uncertainty, fmt='o', alpha=0.5,\n",
        "                    ecolor='lightgray', capsize=0, label='Predictions with Uncertainty')\n",
        "        plt.plot(range(len(y_true)), y_true, 'r.', alpha=0.7, label='True Values')\n",
        "\n",
        "        min_val = min(np.min(y_true), np.min(y_pred))\n",
        "        max_val = max(np.max(y_true), np.max(y_pred))\n",
        "        plt.plot([0, len(y_true)], [min_val, max_val], 'k--', alpha=0.3)\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Sample Index (sorted)')\n",
        "        plt.ylabel('Value')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('bnn_predictions.png')\n",
        "        plt.close()\n",
        "\n",
        "    def plot_energy_performance_tradeoff(energy_values, performance_values):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(energy_values, performance_values, alpha=0.7)\n",
        "        plt.title('Energy-Performance Tradeoff')\n",
        "        plt.xlabel('Energy Consumption')\n",
        "        plt.ylabel('Performance (negative is better)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('energy_performance_tradeoff.png')\n",
        "        plt.close()\n",
        "\n",
        "    def create_sample_edges(n_source, n_target, density=0.1):\n",
        "        n_edges = int(n_source * n_target * density)\n",
        "\n",
        "        sources = torch.randint(0, n_source, (n_edges,))\n",
        "        targets = torch.randint(0, n_target, (n_edges,))\n",
        "\n",
        "        edge_index = torch.stack([sources, targets], dim=0)\n",
        "        return edge_index\n",
        "\n",
        "    def create_bipartite_edges(n_jobs, n_resources, density=0.2):\n",
        "        max_edges = n_jobs * n_resources\n",
        "        n_edges = int(max_edges * density)\n",
        "\n",
        "        sources = list(range(n_jobs))\n",
        "        targets = np.random.randint(0, n_resources, size=n_jobs)\n",
        "\n",
        "        remaining = n_edges - n_jobs\n",
        "        if remaining > 0:\n",
        "            extra_sources = torch.randint(0, n_jobs, (remaining,))\n",
        "            extra_targets = torch.randint(0, n_resources, (remaining,))\n",
        "\n",
        "            sources.extend(extra_sources.tolist())\n",
        "            targets.extend(extra_targets.tolist())\n",
        "\n",
        "        edge_index = torch.tensor([sources, targets], dtype=torch.long)\n",
        "        return edge_index\n",
        "\n",
        "    plot_energy_performance_tradeoff(energy_values, performance_values)\n",
        "\n",
        "    print(\"\\nFinal allocation statistics:\")\n",
        "    print(f\"Total jobs: {n_jobs}\")\n",
        "    print(f\"Total resources: {n_resources}\")\n",
        "    print(f\"Average energy consumption: {energy_values[-1]:.2f} kWh\")\n",
        "    print(f\"Performance metric: {performance_values[-1]:.2f}\")\n",
        "\n",
        "    print(\"\\nHARMONIC Framework Demo Complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnoPjetFRGut"
      },
      "source": [
        "Updated code implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7q9rriYqcSw"
      },
      "source": [
        "Corrected and updated code implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import time\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def load_dataset(filepath):\n",
        "\n",
        "    print(f\"Loading data from {filepath}\")\n",
        "\n",
        "\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "\n",
        "    data = {\n",
        "        'QUEUED_TIMESTAMP': pd.date_range(start='2024-01-01', periods=n_samples, freq='H'),\n",
        "        'START_TIMESTAMP': pd.date_range(start='2024-01-01 01:00:00', periods=n_samples, freq='H'),\n",
        "        'END_TIMESTAMP': pd.date_range(start='2024-01-01 03:00:00', periods=n_samples, freq='H'),\n",
        "        'WALLTIME_SECONDS': np.random.randint(3600, 14400, n_samples),\n",
        "        'RUNTIME_SECONDS': np.random.randint(1800, 10800, n_samples),\n",
        "        'CORES_REQUESTED': np.random.randint(16, 128, n_samples),\n",
        "        'CORES_USED': np.random.randint(16, 128, n_samples),\n",
        "        'NODES_REQUESTED': np.random.randint(1, 32, n_samples),\n",
        "        'NODES_USED': np.random.randint(1, 32, n_samples),\n",
        "        'EXIT_STATUS': np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
        "        'MACHINE_NAME': np.random.choice(['polaris', 'mira', 'cooley'], n_samples),\n",
        "        'PRIORITY': np.random.randint(1, 11, n_samples),\n",
        "        'ENERGY_SENSITIVITY': np.random.beta(2, 5, n_samples)\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "    return preprocess_data(df)\n",
        "\n",
        "def preprocess_data(df):\n",
        "\n",
        "    for col in ['QUEUED_TIMESTAMP', 'START_TIMESTAMP', 'END_TIMESTAMP']:\n",
        "        if df[col].dtype == object:\n",
        "            df[col] = pd.to_datetime(df[col])\n",
        "\n",
        "    df['WAIT_TIME'] = (df['START_TIMESTAMP'] - df['QUEUED_TIMESTAMP']).dt.total_seconds()\n",
        "    df['ACTUAL_RUNTIME'] = (df['END_TIMESTAMP'] - df['START_TIMESTAMP']).dt.total_seconds()\n",
        "\n",
        "    df['RUNTIME_RATIO'] = np.where(df['WALLTIME_SECONDS'] > 0,\n",
        "                                   np.clip(df['RUNTIME_SECONDS'] / df['WALLTIME_SECONDS'], 0, 2),\n",
        "                                   0)\n",
        "    df['CORE_EFFICIENCY'] = np.where(df['CORES_REQUESTED'] > 0,\n",
        "                                    np.clip(df['CORES_USED'] / df['CORES_REQUESTED'], 0, 1),\n",
        "                                    0)\n",
        "    df['NODE_EFFICIENCY'] = np.where(df['NODES_REQUESTED'] > 0,\n",
        "                                    np.clip(df['NODES_USED'] / df['NODES_REQUESTED'], 0, 1),\n",
        "                                    0)\n",
        "\n",
        "\n",
        "    df = df[df['RUNTIME_SECONDS'] > 0]\n",
        "    df = df[df['WAIT_TIME'] >= 0]\n",
        "    df = df[df['RUNTIME_SECONDS'] < df['WALLTIME_SECONDS'] * 3]\n",
        "\n",
        "\n",
        "    node_thresholds = [0,\n",
        "                      df['NODES_USED'].quantile(0.25),\n",
        "                      df['NODES_USED'].quantile(0.5),\n",
        "                      df['NODES_USED'].quantile(0.75),\n",
        "                      float('inf')]\n",
        "\n",
        "    node_thresholds = sorted(list(set([round(x, 3) for x in node_thresholds])))\n",
        "    if len(node_thresholds) < 3:\n",
        "        max_nodes = df['NODES_USED'].max()\n",
        "        node_thresholds = [0, max_nodes/3, 2*max_nodes/3, float('inf')]\n",
        "\n",
        "    size_labels = ['Small', 'Medium', 'Large', 'Very Large'][:len(node_thresholds)-1]\n",
        "\n",
        "    df['JOB_SIZE_CATEGORY'] = pd.cut(\n",
        "        df['NODES_USED'],\n",
        "        bins=node_thresholds,\n",
        "        labels=size_labels,\n",
        "        include_lowest=True\n",
        "    )\n",
        "\n",
        "\n",
        "    runtime_thresholds = [0,\n",
        "                         df['RUNTIME_SECONDS'].quantile(0.25),\n",
        "                         df['RUNTIME_SECONDS'].quantile(0.5),\n",
        "                         df['RUNTIME_SECONDS'].quantile(0.75),\n",
        "                         float('inf')]\n",
        "\n",
        "    runtime_thresholds = sorted(list(set([round(x, 3) for x in runtime_thresholds])))\n",
        "    if len(runtime_thresholds) < 3:\n",
        "        max_runtime = df['RUNTIME_SECONDS'].max()\n",
        "        runtime_thresholds = [0, max_runtime/3, 2*max_runtime/3, float('inf')]\n",
        "\n",
        "    runtime_labels = ['Short', 'Medium', 'Long', 'Very Long'][:len(runtime_thresholds)-1]\n",
        "\n",
        "    df['RUNTIME_CATEGORY'] = pd.cut(\n",
        "        df['RUNTIME_SECONDS'],\n",
        "        bins=runtime_thresholds,\n",
        "        labels=runtime_labels,\n",
        "        include_lowest=True\n",
        "    )\n",
        "\n",
        "    df['RESOURCE_INTENSITY'] = df['CORES_USED'] * df['RUNTIME_SECONDS'] / 3600  # Core-hours\n",
        "\n",
        "    df['HOUR_OF_DAY'] = df['START_TIMESTAMP'].dt.hour\n",
        "    df['DAY_OF_WEEK'] = df['START_TIMESTAMP'].dt.dayofweek\n",
        "    df['IS_WEEKEND'] = df['DAY_OF_WEEK'].isin([5, 6]).astype(int)\n",
        "\n",
        "    peak_hours = (df['HOUR_OF_DAY'] >= 9) & (df['HOUR_OF_DAY'] <= 17) & (df['DAY_OF_WEEK'] < 5)\n",
        "    df['IS_PEAK_HOURS'] = peak_hours.astype(int)\n",
        "\n",
        "    df['IS_COMPLETE'] = (df['EXIT_STATUS'] == 0).astype(int)\n",
        "\n",
        "\n",
        "    base_power = {'polaris': 400, 'mira': 300, 'cooley': 200}\n",
        "    default_power = 300\n",
        "\n",
        "    machine_names = df['MACHINE_NAME'].str.lower()\n",
        "\n",
        "    power_factor = np.where(machine_names == 'polaris', 1.0,\n",
        "                          np.where(machine_names == 'mira', 0.8, 0.6))\n",
        "\n",
        "    machine_power = machine_names.map(lambda x: base_power.get(x, default_power))\n",
        "\n",
        "    df['ENERGY_ESTIMATE'] = df['NODES_USED'] * df['RUNTIME_SECONDS'] * power_factor * \\\n",
        "                          (machine_power / 1000)\n",
        "\n",
        "\n",
        "    df['DEADLINE'] = df['RUNTIME_SECONDS'] * np.random.uniform(1.2, 2.5, size=len(df))\n",
        "\n",
        "\n",
        "    df['PRIORITY'] = np.random.randint(1, 11, size=len(df))\n",
        "\n",
        "\n",
        "    df['ENERGY_SENSITIVITY'] = np.random.beta(2, 5, size=len(df))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(0, 0.01))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).normal_(-5, 0.01))\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(0, 0.01))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).normal_(-5, 0.01))\n",
        "\n",
        "        self.weight_prior_mu = torch.zeros(out_features, in_features)\n",
        "        self.weight_prior_sigma = torch.ones(out_features, in_features)\n",
        "        self.bias_prior_mu = torch.zeros(out_features)\n",
        "        self.bias_prior_sigma = torch.ones(out_features)\n",
        "\n",
        "        self.kl = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "\n",
        "        weight_sigma = torch.clamp(weight_sigma, min=1e-6, max=1e2)\n",
        "        bias_sigma = torch.clamp(bias_sigma, min=1e-6, max=1e2)\n",
        "\n",
        "        weight_epsilon = torch.randn_like(weight_sigma)\n",
        "        bias_epsilon = torch.randn_like(bias_sigma)\n",
        "\n",
        "        weight = self.weight_mu + weight_epsilon * weight_sigma\n",
        "        bias = self.bias_mu + bias_epsilon * bias_sigma\n",
        "\n",
        "        self.kl = self._kl_divergence(weight, self.weight_mu, weight_sigma,\n",
        "                                  self.weight_prior_mu, self.weight_prior_sigma)\n",
        "        self.kl += self._kl_divergence(bias, self.bias_mu, bias_sigma,\n",
        "                                   self.bias_prior_mu, self.bias_prior_sigma)\n",
        "\n",
        "        return F.linear(x, weight, bias)\n",
        "\n",
        "    def _kl_divergence(self, z, mu_q, sigma_q, mu_p, sigma_p):\n",
        "        term1 = torch.log(sigma_p + 1e-8) - torch.log(sigma_q + 1e-8)\n",
        "        term2 = (sigma_q.pow(2) + (mu_q - mu_p).pow(2)) / (2 * sigma_p.pow(2) + 1e-8)\n",
        "        term3 = -0.5\n",
        "\n",
        "        kl = (term1 + term2 + term3).sum()\n",
        "        return torch.clamp(kl, min=0.0, max=1e6)\n",
        "\n",
        "class BayesianNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_hidden=2, dropout_rate=0.1):\n",
        "        super(BayesianNeuralNetwork, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.n_hidden = n_hidden\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList([\n",
        "            nn.Linear(hidden_dim, hidden_dim) for _ in range(n_hidden)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.mean_layer = nn.Linear(hidden_dim, output_dim)\n",
        "        self.log_var_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "        self.prior_mean = torch.tensor(0.0)\n",
        "        self.prior_var = torch.tensor(1.0)\n",
        "\n",
        "    def forward(self, x, num_samples=1):\n",
        "        kl_divergence = 0.0\n",
        "        means = []\n",
        "        log_vars = []\n",
        "\n",
        "        for _ in range(num_samples):\n",
        "\n",
        "            x_i = F.relu(self.dropout(self.input_layer(x)))\n",
        "\n",
        "\n",
        "            for hidden_layer in self.hidden_layers:\n",
        "                x_i = F.relu(self.dropout(hidden_layer(x_i)))\n",
        "\n",
        "\n",
        "            mean = self.mean_layer(x_i)\n",
        "            log_var = self.log_var_layer(x_i)\n",
        "\n",
        "\n",
        "            log_var = torch.clamp(log_var, min=-10.0, max=10.0)\n",
        "\n",
        "            means.append(mean)\n",
        "            log_vars.append(log_var)\n",
        "\n",
        "            for layer in list(self.hidden_layers) + [self.mean_layer, self.log_var_layer]:\n",
        "                weight_mu = layer.weight\n",
        "                weight_sigma = self.dropout_rate * weight_mu.pow(2)\n",
        "\n",
        "\n",
        "                prior_var = self.prior_var.to(weight_mu.device)\n",
        "                prior_mean = self.prior_mean.to(weight_mu.device)\n",
        "\n",
        "                kl_layer = 0.5 * (\n",
        "                    (weight_sigma / prior_var) +\n",
        "                    (prior_mean - weight_mu).pow(2) / prior_var -\n",
        "                    1 +\n",
        "                    torch.log(prior_var) -\n",
        "                    torch.log(weight_sigma + 1e-8)\n",
        "                ).sum()\n",
        "\n",
        "                kl_divergence += kl_layer\n",
        "\n",
        "        mean_pred = torch.stack(means).mean(0)\n",
        "        var_pred = torch.exp(torch.stack(log_vars)).mean(0) + torch.var(torch.stack(means), dim=0)\n",
        "\n",
        "        var_pred = torch.clamp(var_pred, min=1e-6)\n",
        "\n",
        "        return mean_pred, var_pred, kl_divergence\n",
        "\n",
        "class GraphNetwork(nn.Module):\n",
        "    def __init__(self, job_features, resource_features, edge_features, hidden_dim, output_dim):\n",
        "        super(GraphNetwork, self).__init__()\n",
        "        self.job_encoder = nn.Sequential(\n",
        "            nn.Linear(job_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.resource_encoder = nn.Sequential(\n",
        "            nn.Linear(resource_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(edge_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.job_gcn = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.resource_gcn = GCNConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.job_gat = GATConv(hidden_dim, hidden_dim // 4, heads=4)\n",
        "        self.resource_gat = GATConv(hidden_dim, hidden_dim // 4, heads=4)\n",
        "\n",
        "        self.job_classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, job_features, resource_features, edge_index, edge_attr=None):\n",
        "\n",
        "        job_embeds = self.job_encoder(job_features)\n",
        "        resource_embeds = self.resource_encoder(resource_features)\n",
        "\n",
        "\n",
        "        if edge_attr is not None:\n",
        "            edge_embeds = self.edge_encoder(edge_attr)\n",
        "        else:\n",
        "            edge_embeds = None\n",
        "\n",
        "\n",
        "        job_gcn_embeds = F.relu(self.job_gcn(job_embeds, edge_index))\n",
        "        resource_gcn_embeds = F.relu(self.resource_gcn(resource_embeds, edge_index[[1, 0]]))  # Reversed edge_index\n",
        "\n",
        "\n",
        "        job_gat_embeds = F.relu(self.job_gat(job_embeds, edge_index))\n",
        "        resource_gat_embeds = F.relu(self.resource_gat(resource_embeds, edge_index[[1, 0]]))\n",
        "\n",
        "\n",
        "        job_final_embeds = torch.cat([job_gcn_embeds, job_gat_embeds], dim=1)\n",
        "\n",
        "        job_outputs = self.job_classifier(job_final_embeds)\n",
        "\n",
        "        return job_outputs\n",
        "\n",
        "\n",
        "class JobSchedulerEnvironment:\n",
        "    def __init__(self, jobs_data, resources_data, energy_weight=0.3, deadline_weight=0.5, resilience_weight=0.2,\n",
        "                 max_steps=1000):\n",
        "        self.jobs = jobs_data\n",
        "        self.resources = resources_data\n",
        "        self.n_jobs = len(jobs_data)\n",
        "        self.n_resources = len(resources_data)\n",
        "        self.max_steps = max_steps\n",
        "        self.steps_taken = 0\n",
        "\n",
        "        self.energy_weight = energy_weight\n",
        "        self.deadline_weight = deadline_weight\n",
        "        self.resilience_weight = resilience_weight\n",
        "\n",
        "        self.time = 0\n",
        "        self.assigned_jobs = {}\n",
        "        self.completed_jobs = set()\n",
        "        self.resource_available_time = [0] * self.n_resources\n",
        "\n",
        "\n",
        "        self.energy_consumption = 0\n",
        "        self.deadline_violations = 0\n",
        "        self.system_reliability = 1.0\n",
        "\n",
        "    def reset(self):\n",
        "        self.time = 0\n",
        "        self.assigned_jobs = {}\n",
        "        self.completed_jobs = set()\n",
        "        self.resource_available_time = [0] * self.n_resources\n",
        "        self.energy_consumption = 0\n",
        "        self.deadline_violations = 0\n",
        "        self.system_reliability = 1.0\n",
        "        self.steps_taken = 0\n",
        "\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        job_features = []\n",
        "        for job_id in range(self.n_jobs):\n",
        "            if job_id in self.completed_jobs or job_id in self.assigned_jobs:\n",
        "                job_features.append([0, 0, 0, 0, 0])\n",
        "            else:\n",
        "                job_features.append([\n",
        "                    self.jobs[job_id]['deadline'],\n",
        "                    self.jobs[job_id]['runtime'],\n",
        "                    self.jobs[job_id]['size'],\n",
        "                    self.jobs[job_id]['priority'],\n",
        "                    self.jobs[job_id]['energy_sensitivity']\n",
        "                ])\n",
        "\n",
        "        resource_features = []\n",
        "        for res_id in range(self.n_resources):\n",
        "            resource_features.append([\n",
        "                max(0, self.resource_available_time[res_id] - self.time),\n",
        "                self.resources[res_id]['reliability'],\n",
        "                self.resources[res_id]['energy_efficiency']\n",
        "            ])\n",
        "\n",
        "\n",
        "        global_features = [\n",
        "            self.time,\n",
        "            self.energy_consumption,\n",
        "            self.deadline_violations,\n",
        "            self.system_reliability,\n",
        "            len(self.completed_jobs) / max(1, self.n_jobs)\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'job_features': torch.tensor(job_features, dtype=torch.float32),\n",
        "            'resource_features': torch.tensor(resource_features, dtype=torch.float32),\n",
        "            'global_features': torch.tensor(global_features, dtype=torch.float32),\n",
        "            'mask': [job_id not in self.completed_jobs and job_id not in self.assigned_jobs\n",
        "                    for job_id in range(self.n_jobs)]\n",
        "        }\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps_taken += 1\n",
        "\n",
        "\n",
        "        if self.steps_taken >= self.max_steps:\n",
        "            print(f\"Environment reached maximum steps ({self.max_steps}). Terminating.\")\n",
        "            return self._get_state(), -1, True, {'error': 'Max steps reached'}\n",
        "\n",
        "        job_id, resource_id, speed = action\n",
        "\n",
        "\n",
        "        if job_id in self.completed_jobs or job_id in self.assigned_jobs:\n",
        "            return self._get_state(), -100, False, {'error': 'Invalid job selection'}\n",
        "\n",
        "\n",
        "        start_time = max(self.time, self.resource_available_time[resource_id])\n",
        "\n",
        "\n",
        "        baseline_runtime = self.jobs[job_id]['runtime']\n",
        "\n",
        "        adjusted_runtime = baseline_runtime / speed\n",
        "\n",
        "\n",
        "        completion_time = start_time + adjusted_runtime\n",
        "\n",
        "\n",
        "        self.resource_available_time[resource_id] = completion_time\n",
        "\n",
        "        energy_efficiency = self.resources[resource_id]['energy_efficiency']\n",
        "        job_size = self.jobs[job_id]['size']\n",
        "        energy_consumed = job_size * adjusted_runtime * (speed ** 2) * (1 / energy_efficiency)\n",
        "        self.energy_consumption += energy_consumed\n",
        "\n",
        "\n",
        "        deadline = self.jobs[job_id]['deadline']\n",
        "        deadline_violation = max(0, completion_time - deadline)\n",
        "        if deadline_violation > 0:\n",
        "            self.deadline_violations += 1\n",
        "\n",
        "\n",
        "        resource_reliability = self.resources[resource_id]['reliability']\n",
        "        job_sensitivity = self.jobs[job_id]['energy_sensitivity']\n",
        "        reliability_factor = resource_reliability * (1 - job_sensitivity * (speed - 1)**2)\n",
        "        reliability_factor = max(0.5, min(1.0, reliability_factor))\n",
        "        self.system_reliability *= reliability_factor\n",
        "\n",
        "        self.assigned_jobs[job_id] = (resource_id, start_time, speed)\n",
        "        self.completed_jobs.add(job_id)\n",
        "\n",
        "        if len(self.assigned_jobs) == self.n_jobs:\n",
        "\n",
        "            done = True\n",
        "        else:\n",
        "            if len(self.resource_available_time) > 0:\n",
        "                next_completion = min(self.resource_available_time)\n",
        "                self.time = next_completion\n",
        "            done = False\n",
        "\n",
        "\n",
        "        energy_term = -energy_consumed / (max(1, job_size * baseline_runtime))\n",
        "        deadline_term = -deadline_violation / max(1, deadline) if deadline > 0 else 0\n",
        "        reliability_term = reliability_factor - 0.5\n",
        "\n",
        "        reward = (self.energy_weight * energy_term +\n",
        "                 self.deadline_weight * deadline_term +\n",
        "                 self.resilience_weight * reliability_term)\n",
        "\n",
        "\n",
        "        return self._get_state(), reward, done, {\n",
        "            'energy': energy_consumed,\n",
        "            'deadline_violation': deadline_violation > 0,\n",
        "            'reliability': reliability_factor\n",
        "        }\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch_size = min(batch_size, len(self.buffer))\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class DQNScheduler(nn.Module):\n",
        "    def __init__(self, job_feature_dim, resource_feature_dim, global_feature_dim, hidden_dim=128):\n",
        "        super(DQNScheduler, self).__init__()\n",
        "\n",
        "        self.job_encoder = nn.Sequential(\n",
        "            nn.Linear(job_feature_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.resource_encoder = nn.Sequential(\n",
        "            nn.Linear(resource_feature_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.global_encoder = nn.Sequential(\n",
        "            nn.Linear(global_feature_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.compatibility_net = nn.Bilinear(hidden_dim, hidden_dim, hidden_dim)\n",
        "\n",
        "\n",
        "        self.q_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 3, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 3)\n",
        "\n",
        "    def forward(self, state):\n",
        "        job_features = state['job_features']\n",
        "        resource_features = state['resource_features']\n",
        "        global_features = state['global_features']\n",
        "        mask = state['mask']\n",
        "\n",
        "        if len(job_features.shape) == 2:\n",
        "            job_features = job_features.unsqueeze(0)\n",
        "        if len(resource_features.shape) == 2:\n",
        "            resource_features = resource_features.unsqueeze(0)\n",
        "        if len(global_features.shape) == 1:\n",
        "            global_features = global_features.unsqueeze(0)\n",
        "\n",
        "        batch_size = job_features.shape[0]\n",
        "        n_jobs = job_features.shape[1]\n",
        "        n_resources = resource_features.shape[1]\n",
        "\n",
        "\n",
        "        job_embeds = self.job_encoder(job_features.reshape(-1, job_features.shape[-1]))\n",
        "        job_embeds = job_embeds.reshape(batch_size, n_jobs, -1)\n",
        "\n",
        "        resource_embeds = self.resource_encoder(resource_features.reshape(-1, resource_features.shape[-1]))\n",
        "        resource_embeds = resource_embeds.reshape(batch_size, n_resources, -1)\n",
        "\n",
        "        global_embed = self.global_encoder(global_features)\n",
        "        global_embed_expanded = global_embed.unsqueeze(1).unsqueeze(1).expand(batch_size, n_jobs, n_resources, -1)\n",
        "\n",
        "\n",
        "        q_values = torch.zeros(batch_size, n_jobs, n_resources, 3, device=job_features.device)\n",
        "\n",
        "        for j in range(n_jobs):\n",
        "            if j >= job_embeds.shape[1]:\n",
        "                continue\n",
        "\n",
        "            job_embed = job_embeds[:, j, :]\n",
        "            job_embed_expanded = job_embed.unsqueeze(1).expand(batch_size, n_resources, -1)\n",
        "\n",
        "            for r in range(n_resources):\n",
        "                if r >= resource_embeds.shape[1]:\n",
        "                    continue\n",
        "\n",
        "                resource_embed = resource_embeds[:, r, :]\n",
        "                resource_embed_expanded = resource_embed.unsqueeze(1).expand(batch_size, 1, -1)\n",
        "\n",
        "\n",
        "                compatibility = self.compatibility_net(job_embed, resource_embed)\n",
        "\n",
        "                combined = torch.cat([\n",
        "                    compatibility,\n",
        "                    job_embed,\n",
        "                    resource_embed,\n",
        "                ], dim=1)  ]\n",
        "\n",
        "\n",
        "                q_vals = self.q_net(combined)\n",
        "                q_values[:, j, r, :] = q_vals\n",
        "\n",
        "\n",
        "        if isinstance(mask, list):\n",
        "            try:\n",
        "                mask = torch.tensor(mask, dtype=torch.bool, device=job_features.device)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting mask to tensor: {e}\")\n",
        "\n",
        "                mask = torch.ones((batch_size, n_jobs), dtype=torch.bool, device=job_features.device)\n",
        "\n",
        "\n",
        "        if mask.dim() == 1:\n",
        "            mask = mask.reshape(1, -1)\n",
        "        if mask.shape[1] != n_jobs:\n",
        "\n",
        "            new_mask = torch.ones((batch_size, n_jobs), dtype=torch.bool, device=job_features.device)\n",
        "            new_mask[:, :min(mask.shape[1], n_jobs)] = mask[:, :min(mask.shape[1], n_jobs)]\n",
        "            mask = new_mask\n",
        "\n",
        "\n",
        "        try:\n",
        "            mask_expanded = mask.reshape(batch_size, n_jobs, 1, 1).expand(-1, -1, n_resources, 3)\n",
        "            q_values = torch.where(mask_expanded, q_values, torch.tensor(-1e9, device=job_features.device))\n",
        "        except Exception as e:\n",
        "            print(f\"Error applying mask: {e}\")\n",
        "\n",
        "\n",
        "        return q_values\n",
        "\n",
        "\n",
        "def train_dqn_scheduler(env, model, target_model, num_episodes=1000, batch_size=64, gamma=0.99,\n",
        "                     epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
        "                     target_update=10, learning_rate=0.001, buffer_capacity=10000):\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "\n",
        "    rewards_history = []\n",
        "    energy_history = []\n",
        "    deadline_violations_history = []\n",
        "    reliability_history = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    target_model.to(device)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_energy = 0\n",
        "        episode_violations = 0\n",
        "        episode_reliability = 1.0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            if random.random() < epsilon:\n",
        "\n",
        "                available_jobs = [i for i, m in enumerate(state['mask']) if m]\n",
        "                if not available_jobs:\n",
        "                    break\n",
        "\n",
        "                job_id = random.choice(available_jobs)\n",
        "                resource_id = random.randint(0, env.n_resources - 1)\n",
        "                speed_idx = random.randint(0, 2)\n",
        "                speed = [0.8, 1.0, 1.2][speed_idx]\n",
        "                action = (job_id, resource_id, speed)\n",
        "            else:\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    state_tensors = {\n",
        "                        'job_features': torch.as_tensor(state['job_features'], dtype=torch.float32).to(device),\n",
        "                        'resource_features': torch.as_tensor(state['resource_features'], dtype=torch.float32).to(device),\n",
        "                        'global_features': torch.as_tensor(state['global_features'], dtype=torch.float32).to(device),\n",
        "                        'mask': state['mask']\n",
        "                    }\n",
        "                    q_values = model(state_tensors)\n",
        "\n",
        "                    job_id, resource_id, speed_idx = np.unravel_index(\n",
        "                        torch.argmax(q_values).cpu().item(),\n",
        "                        q_values.shape[1:])\n",
        "                    speed = [0.8, 1.0, 1.2][speed_idx]\n",
        "                    action = (job_id, resource_id, speed)\n",
        "\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            replay_buffer.push(state, action, reward, next_state, done)\n",
        "\n",
        "\n",
        "            if len(replay_buffer) > batch_size:\n",
        "\n",
        "                batch = replay_buffer.sample(batch_size)\n",
        "\n",
        "\n",
        "                batch_states = [s for s, _, _, _, _ in batch]\n",
        "                batch_actions = [a for _, a, _, _, _ in batch]\n",
        "                batch_rewards = torch.as_tensor([r for _, _, r, _, _ in batch], dtype=torch.float32).to(device)\n",
        "                batch_next_states = [s for _, _, _, s, _ in batch]\n",
        "                batch_dones = torch.as_tensor([d for _, _, _, _, d in batch], dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "                processed_states = []\n",
        "                for s in batch_states:\n",
        "                    processed_states.append({\n",
        "                        'job_features': torch.as_tensor(s['job_features'], dtype=torch.float32).to(device),\n",
        "                        'resource_features': torch.as_tensor(s['resource_features'], dtype=torch.float32).to(device),\n",
        "                        'global_features': torch.as_tensor(s['global_features'], dtype=torch.float32).to(device),\n",
        "                        'mask': s['mask']\n",
        "                    })\n",
        "\n",
        "                processed_next_states = []\n",
        "                for s in batch_next_states:\n",
        "                    processed_next_states.append({\n",
        "                        'job_features': torch.as_tensor(s['job_features'], dtype=torch.float32).to(device),\n",
        "                        'resource_features': torch.as_tensor(s['resource_features'], dtype=torch.float32).to(device),\n",
        "                        'global_features': torch.as_tensor(s['global_features'], dtype=torch.float32).to(device),\n",
        "                        'mask': s['mask']\n",
        "                    })\n",
        "\n",
        "                current_q_values = []\n",
        "                for i, state in enumerate(processed_states):\n",
        "                    q_vals = model(state)\n",
        "                    job_id, resource_id, speed_idx = batch_actions[i]\n",
        "\n",
        "\n",
        "                    job_id = int(job_id)\n",
        "                    resource_id = int(resource_id)\n",
        "                    speed_idx = int(speed_idx)\n",
        "\n",
        "                    current_q_values.append(q_vals[0, job_id, resource_id, speed_idx])\n",
        "                current_q_values = torch.stack(current_q_values)\n",
        "\n",
        "\n",
        "                next_q_values = []\n",
        "                for state in processed_next_states:\n",
        "                    with torch.no_grad():\n",
        "                        q_vals = target_model(state)\n",
        "                        next_q_values.append(torch.max(q_vals))\n",
        "                next_q_values = torch.stack(next_q_values)\n",
        "\n",
        "\n",
        "                target_q_values = batch_rewards + (1 - batch_dones) * gamma * next_q_values\n",
        "\n",
        "                loss = F.mse_loss(current_q_values, target_q_values)\n",
        "\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            episode_energy += info['energy']\n",
        "            episode_violations += int(info['deadline_violation'])\n",
        "            episode_reliability *= info['reliability']\n",
        "\n",
        "\n",
        "        if episode % target_update == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "\n",
        "\n",
        "        rewards_history.append(episode_reward)\n",
        "        energy_history.append(episode_energy)\n",
        "        deadline_violations_history.append(episode_violations)\n",
        "        reliability_history.append(episode_reliability)\n",
        "\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode}: Reward = {episode_reward:.2f}, Energy = {episode_energy:.2f}, \"\n",
        "                  f\"Violations = {episode_violations}, Reliability = {episode_reliability:.4f}\")\n",
        "\n",
        "\n",
        "            if episode % 100 == 0 and episode > 0:\n",
        "                plot_learning_curves(\n",
        "                    rewards_history, energy_history,\n",
        "                    deadline_violations_history, reliability_history\n",
        "                )\n",
        "\n",
        "    return model, {\n",
        "        'rewards': rewards_history,\n",
        "        'energy': energy_history,\n",
        "        'violations': deadline_violations_history,\n",
        "        'reliability': reliability_history\n",
        "    }\n",
        "\n",
        "class ParticleSwarmOptimizer:\n",
        "    def __init__(self, n_particles, n_dimensions, bounds, objectives, weights, inertia=0.5,\n",
        "                 cognitive=1.5, social=1.5):\n",
        "        self.n_particles = n_particles\n",
        "        self.n_dimensions = n_dimensions\n",
        "        self.bounds = bounds\n",
        "        self.objectives = objectives\n",
        "        self.weights = weights\n",
        "        self.inertia = inertia\n",
        "        self.cognitive = cognitive\n",
        "        self.social = social\n",
        "\n",
        "        self.positions = np.zeros((n_particles, n_dimensions))\n",
        "        for i in range(n_dimensions):\n",
        "            self.positions[:, i] = np.random.uniform(bounds[i][0], bounds[i][1], n_particles)\n",
        "\n",
        "        self.velocities = np.zeros((n_particles, n_dimensions))\n",
        "        for i in range(n_dimensions):\n",
        "            range_width = bounds[i][1] - bounds[i][0]\n",
        "            self.velocities[:, i] = np.random.uniform(-range_width/10, range_width/10, n_particles)\n",
        "\n",
        "        self.personal_best_positions = self.positions.copy()\n",
        "        self.personal_best_fitness = np.full(n_particles, float('inf'))\n",
        "\n",
        "        self.global_best_position = np.zeros(n_dimensions)\n",
        "        self.global_best_fitness = float('inf')\n",
        "\n",
        "        self.objective_history = []\n",
        "\n",
        "        self._evaluate_all_particles()\n",
        "\n",
        "    def _evaluate_particle(self, position):\n",
        "        objective_values = [obj(position) for obj in self.objectives]\n",
        "        weighted_sum = sum(w * obj for w, obj in zip(self.weights, objective_values))\n",
        "        return weighted_sum, objective_values\n",
        "\n",
        "    def _evaluate_all_particles(self):\n",
        "        for i in range(self.n_particles):\n",
        "            fitness, objective_values = self._evaluate_particle(self.positions[i])\n",
        "\n",
        "            if fitness < self.personal_best_fitness[i]:\n",
        "                self.personal_best_fitness[i] = fitness\n",
        "                self.personal_best_positions[i] = self.positions[i].copy()\n",
        "\n",
        "            if fitness < self.global_best_fitness:\n",
        "                self.global_best_fitness = fitness\n",
        "                self.global_best_position = self.positions[i].copy()\n",
        "                self.objective_history.append(objective_values)\n",
        "\n",
        "    def optimize(self, max_iterations=100):\n",
        "        all_positions = []\n",
        "        all_objectives = []\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            r1 = np.random.random((self.n_particles, self.n_dimensions))\n",
        "            r2 = np.random.random((self.n_particles, self.n_dimensions))\n",
        "\n",
        "            cognitive_component = self.cognitive * r1 * (self.personal_best_positions - self.positions)\n",
        "            social_component = self.social * r2 * (self.global_best_position - self.positions)\n",
        "\n",
        "            self.velocities = self.inertia * self.velocities + cognitive_component + social_component\n",
        "\n",
        "            for i in range(self.n_dimensions):\n",
        "                range_width = self.bounds[i][1] - self.bounds[i][0]\n",
        "                max_velocity = range_width / 5\n",
        "                self.velocities[:, i] = np.clip(self.velocities[:, i], -max_velocity, max_velocity)\n",
        "\n",
        "            self.positions += self.velocities\n",
        "\n",
        "            for i in range(self.n_dimensions):\n",
        "                self.positions[:, i] = np.clip(self.positions[:, i], self.bounds[i][0], self.bounds[i][1])\n",
        "\n",
        "            self._evaluate_all_particles()\n",
        "\n",
        "            for i in range(self.n_particles):\n",
        "                all_positions.append(self.positions[i].copy())\n",
        "                _, objs = self._evaluate_particle(self.positions[i])\n",
        "                all_objectives.append(objs)\n",
        "\n",
        "            if iteration % 10 == 0:\n",
        "                print(f\"Iteration {iteration}: Best fitness = {self.global_best_fitness}\")\n",
        "                print(f\"Objectives: {self.objective_history[-1]}\")\n",
        "\n",
        "        return self.global_best_position, all_positions, all_objectives\n",
        "\n",
        "    def plot_predictions_with_uncertainty(y_true, y_pred, uncertainty, title=\"Predictions with Uncertainty\"):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        sorted_indices = np.argsort(y_true.flatten())\n",
        "        y_true_sorted = y_true[sorted_indices]\n",
        "        y_pred_sorted = y_pred[sorted_indices]\n",
        "        uncertainty_sorted = uncertainty[sorted_indices]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(y_true_sorted, 'b-', label='True Values')\n",
        "        plt.plot(y_pred_sorted, 'r-', label='Predictions')\n",
        "        plt.fill_between(\n",
        "            np.arange(len(y_pred_sorted)),\n",
        "            y_pred_sorted.flatten() - 2 * uncertainty_sorted.flatten(),\n",
        "            y_pred_sorted.flatten() + 2 * uncertainty_sorted.flatten(),\n",
        "            alpha=0.2, color='r', label='95% Confidence'\n",
        "        )\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Sample')\n",
        "        plt.ylabel('Value')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_learning_curves(rewards, energy, violations, reliability):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "        window = 10\n",
        "        rewards_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "        energy_avg = np.convolve(energy, np.ones(window)/window, mode='valid')\n",
        "        violations_avg = np.convolve(violations, np.ones(window)/window, mode='valid')\n",
        "        reliability_avg = np.convolve(reliability, np.ones(window)/window, mode='valid')\n",
        "\n",
        "        axs[0, 0].plot(rewards_avg)\n",
        "        axs[0, 0].set_title('Average Reward')\n",
        "        axs[0, 0].set_xlabel('Episode')\n",
        "        axs[0, 0].set_ylabel('Reward')\n",
        "\n",
        "        axs[0, 1].plot(energy_avg)\n",
        "        axs[0, 1].set_title('Energy Consumption')\n",
        "        axs[0, 1].set_xlabel('Episode')\n",
        "        axs[0, 1].set_ylabel('Energy')\n",
        "\n",
        "        axs[1, 0].plot(violations_avg)\n",
        "        axs[1, 0].set_title('Deadline Violations')\n",
        "        axs[1, 0].set_xlabel('Episode')\n",
        "        axs[1, 0].set_ylabel('Violations')\n",
        "\n",
        "        axs[1, 1].plot(reliability_avg)\n",
        "        axs[1, 1].set_title('System Reliability')\n",
        "        axs[1, 1].set_xlabel('Episode')\n",
        "        axs[1, 1].set_ylabel('Reliability')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_pareto_front(positions, objectives):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        positions = np.array(positions)\n",
        "        objectives = np.array(objectives)\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 8))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        scatter = ax.scatter(\n",
        "            objectives[:, 0],\n",
        "            objectives[:, 1],\n",
        "            objectives[:, 2],\n",
        "            c=np.sum(objectives, axis=1),\n",
        "            cmap='viridis',\n",
        "            alpha=0.7\n",
        "        )\n",
        "\n",
        "        cbar = plt.colorbar(scatter)\n",
        "        cbar.set_label('Combined Objective Value')\n",
        "\n",
        "        ax.set_xlabel('Energy Consumption')\n",
        "        ax.set_ylabel('Deadline Violations')\n",
        "        ax.set_zlabel('Reliability Loss')\n",
        "\n",
        "        plt.title('Pareto Front for Multi-Objective Scheduling')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def compute_bnn_loss(mean_pred, var_pred, target, kl_divergence, kl_weight):\n",
        "    \"\"\"\n",
        "    Compute BNN loss with proper numerical stability\n",
        "    \"\"\"\n",
        "    # Ensure positive variance\n",
        "    var_pred = torch.clamp(var_pred, min=1e-6)\n",
        "\n",
        "    # NLL with numerical stability\n",
        "    nll = 0.5 * (\n",
        "        torch.log(var_pred) +\n",
        "        ((target - mean_pred).pow(2) / var_pred)\n",
        "    ).mean()\n",
        "\n",
        "    loss = nll + kl_weight * kl_divergence\n",
        "\n",
        "    return loss, nll\n",
        "\n",
        "def main():\n",
        "    df = load_dataset(\"ANL-ALCF-DJC-POLARIS_20240101_20241031.csv.gz\")\n",
        "\n",
        "    X = df[['NODES_USED', 'CORES_USED', 'WALLTIME_SECONDS', 'PRIORITY', 'ENERGY_SENSITIVITY']].values\n",
        "    y = df['RUNTIME_SECONDS'].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    y_scaler = StandardScaler()\n",
        "    y_train = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "    y_test = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    hidden_dim = 64\n",
        "    output_dim = 1\n",
        "\n",
        "    bnn_model = BayesianNeuralNetwork(input_dim, hidden_dim, output_dim, n_hidden=2, dropout_rate=0.1)\n",
        "    optimizer = optim.Adam(bnn_model.parameters(), lr=0.001)\n",
        "\n",
        "    n_epochs = 50\n",
        "    batch_size = 64\n",
        "    kl_weight = 1.0 / len(X_train)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        permutation = torch.randperm(X_train_tensor.size()[0])\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i in range(0, X_train_tensor.size()[0], batch_size):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            indices = permutation[i:i+batch_size]\n",
        "            batch_x = X_train_tensor[indices]\n",
        "            batch_y = y_train_tensor[indices]\n",
        "\n",
        "            mean, var, kl = bnn_model(batch_x, num_samples=5)\n",
        "\n",
        "            loss, nll = compute_bnn_loss(mean, var, batch_y, kl, kl_weight)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {running_loss}, NLL: {nll.item()}, KL: {kl.item()}\")\n",
        "\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mean_pred, var_pred, _ = bnn_model(X_test_tensor, num_samples=20)\n",
        "        uncertainty = torch.sqrt(var_pred).detach().numpy()\n",
        "        mean_pred = mean_pred.detach().numpy()\n",
        "\n",
        "    mean_pred = y_scaler.inverse_transform(mean_pred)\n",
        "    uncertainty = uncertainty * y_scaler.scale_\n",
        "    y_test_orig = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "    plot_predictions_with_uncertainty(\n",
        "        y_test_orig.reshape(-1, 1),\n",
        "        mean_pred,\n",
        "        uncertainty,\n",
        "        title=\"Runtime Prediction with Uncertainty\"\n",
        "    )\n",
        "\n",
        "    n_resources = 10\n",
        "    resources_data = []\n",
        "    for i in range(n_resources):\n",
        "        reliability = 0.9 + 0.1 * np.random.beta(5, 2)\n",
        "        energy_efficiency = 0.7 + 0.3 * np.random.beta(2, 5)\n",
        "\n",
        "        resources_data.append({\n",
        "            'id': i,\n",
        "            'reliability': reliability,\n",
        "            'energy_efficiency': energy_efficiency,\n",
        "            'cores': np.random.randint(16, 64)\n",
        "        })\n",
        "\n",
        "    jobs_data = []\n",
        "    for i, row in df.head(100).iterrows():\n",
        "        jobs_data.append({\n",
        "            'id': i,\n",
        "            'runtime': row['RUNTIME_SECONDS'],\n",
        "            'deadline': row['RUNTIME_SECONDS'] * (1.5 + 0.5 * np.random.random()),  # Deadline is 1.5-2x runtime\n",
        "            'size': row['NODES_USED'] * row['CORES_USED'],\n",
        "            'priority': row['PRIORITY'],\n",
        "            'energy_sensitivity': row['ENERGY_SENSITIVITY'] if 'ENERGY_SENSITIVITY' in row else np.random.uniform(0.3, 0.9)\n",
        "        })\n",
        "\n",
        "    env = JobSchedulerEnvironment(jobs_data, resources_data)\n",
        "\n",
        "    job_feature_dim = 5\n",
        "    resource_feature_dim = 3\n",
        "    global_feature_dim = 5\n",
        "\n",
        "    dqn_model = DQNScheduler(job_feature_dim, resource_feature_dim, global_feature_dim)\n",
        "    target_model = DQNScheduler(job_feature_dim, resource_feature_dim, global_feature_dim)\n",
        "    target_model.load_state_dict(dqn_model.state_dict())\n",
        "\n",
        "    trained_model, training_metrics = train_dqn_scheduler(\n",
        "        env, dqn_model, target_model,\n",
        "        num_episodes=500,\n",
        "        batch_size=32,\n",
        "        gamma=0.99,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.01,\n",
        "        epsilon_decay=0.995\n",
        "    )\n",
        "\n",
        "    def energy_objective(params):\n",
        "        energy_weight, deadline_weight, resilience_weight = params\n",
        "        total = energy_weight + deadline_weight + resilience_weight\n",
        "        energy_weight /= total\n",
        "        deadline_weight /= total\n",
        "        resilience_weight /= total\n",
        "\n",
        "        test_env = JobSchedulerEnvironment(\n",
        "            jobs_data, resources_data,\n",
        "            energy_weight=energy_weight,\n",
        "            deadline_weight=deadline_weight,\n",
        "            resilience_weight=resilience_weight\n",
        "        )\n",
        "\n",
        "        state = test_env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                q_values = trained_model(state)\n",
        "                job_id, resource_id, speed_idx = np.unravel_index(\n",
        "                    torch.argmax(q_values).item(),\n",
        "                    q_values.shape[1:])\n",
        "                speed = [0.8, 1.0, 1.2][speed_idx]\n",
        "                action = (job_id, resource_id, speed)\n",
        "\n",
        "            state, reward, done, info = test_env.step(action)\n",
        "\n",
        "        return test_env.energy_consumption\n",
        "\n",
        "    def deadline_objective(params):\n",
        "        energy_weight, deadline_weight, resilience_weight = params\n",
        "        total = energy_weight + deadline_weight + resilience_weight\n",
        "        energy_weight /= total\n",
        "        deadline_weight /= total\n",
        "        resilience_weight /= total\n",
        "\n",
        "        test_env = JobSchedulerEnvironment(\n",
        "            jobs_data, resources_data,\n",
        "            energy_weight=energy_weight,\n",
        "            deadline_weight=deadline_weight,\n",
        "            resilience_weight=resilience_weight\n",
        "        )\n",
        "\n",
        "        state = test_env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                q_values = trained_model(state)\n",
        "                job_id, resource_id, speed_idx = np.unravel_index(\n",
        "                    torch.argmax(q_values).item(),\n",
        "                    q_values.shape[1:])\n",
        "                speed = [0.8, 1.0, 1.2][speed_idx]\n",
        "                action = (job_id, resource_id, speed)\n",
        "\n",
        "            state, reward, done, info = test_env.step(action)\n",
        "\n",
        "        return test_env.deadline_violations\n",
        "\n",
        "    def reliability_objective(params):\n",
        "        energy_weight, deadline_weight, resilience_weight = params\n",
        "        total = energy_weight + deadline_weight + resilience_weight\n",
        "        energy_weight /= total\n",
        "        deadline_weight /= total\n",
        "        resilience_weight /= total\n",
        "\n",
        "        test_env = JobSchedulerEnvironment(\n",
        "            jobs_data, resources_data,\n",
        "            energy_weight=energy_weight,\n",
        "            deadline_weight=deadline_weight,\n",
        "            resilience_weight=resilience_weight\n",
        "        )\n",
        "\n",
        "        state = test_env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            with torch.no_grad():\n",
        "                q_values = trained_model(state)\n",
        "                job_id, resource_id, speed_idx = np.unravel_index(\n",
        "                    torch.argmax(q_values).item(),\n",
        "                    q_values.shape[1:])\n",
        "                speed = [0.8, 1.0, 1.2][speed_idx]\n",
        "                action = (job_id, resource_id, speed)\n",
        "\n",
        "            state, reward, done, info = test_env.step(action)\n",
        "\n",
        "        return 1.0 - test_env.system_reliability\n",
        "\n",
        "    pso = ParticleSwarmOptimizer(\n",
        "        n_particles=20,\n",
        "        n_dimensions=3,\n",
        "        bounds=[(0.1, 1.0), (0.1, 1.0), (0.1, 1.0)],\n",
        "        objectives=[energy_objective, deadline_objective, reliability_objective],\n",
        "        weights=[0.4, 0.4, 0.2],\n",
        "        inertia=0.5,\n",
        "        cognitive=1.5,\n",
        "        social=1.5\n",
        "    )\n",
        "\n",
        "    best_weights, all_positions, all_objectives = pso.optimize(max_iterations=50)\n",
        "\n",
        "    best_weights = best_weights / np.sum(best_weights)\n",
        "    print(f\"Optimized weights: Energy={best_weights[0]:.2f}, Deadline={best_weights[1]:.2f}, Resilience={best_weights[2]:.2f}\")\n",
        "\n",
        "    plot_pareto_front(all_positions, all_objectives)\n",
        "\n",
        "    job_features = np.array([[job['runtime'], job['size'], job['priority'], job['energy_sensitivity']]\n",
        "                            for job in jobs_data])\n",
        "    resource_features = np.array([[res['reliability'], res['energy_efficiency'], res['cores']]\n",
        "                                for res in resources_data])\n",
        "\n",
        "    job_features = (job_features - job_features.mean(axis=0)) / job_features.std(axis=0)\n",
        "    resource_features = (resource_features - resource_features.mean(axis=0)) / resource_features.std(axis=0)\n",
        "\n",
        "    job_features_tensor = torch.tensor(job_features, dtype=torch.float32)\n",
        "    resource_features_tensor = torch.tensor(resource_features, dtype=torch.float32)\n",
        "\n",
        "    graph_model = GraphNetwork(\n",
        "        job_features=job_features.shape[1],\n",
        "        resource_features=resource_features.shape[1],\n",
        "        edge_features=1,\n",
        "        hidden_dim=64,\n",
        "        output_dim=1\n",
        "    )\n",
        "\n",
        "    n_jobs = len(jobs_data)\n",
        "    n_resources = len(resources_data)\n",
        "    edge_index = []\n",
        "    for j in range(n_jobs):\n",
        "        for r in range(n_resources):\n",
        "            edge_index.append([j, n_jobs + r])\n",
        "            edge_index.append([n_jobs + r, j])\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
        "\n",
        "    job_outputs = graph_model(job_features_tensor, resource_features_tensor, edge_index)\n",
        "\n",
        "    affinity_matrix = job_outputs.reshape(n_jobs, n_resources)\n",
        "\n",
        "    print(\"Job-Resource Affinity Matrix:\")\n",
        "    print(affinity_matrix)\n",
        "\n",
        "    final_env = JobSchedulerEnvironment(\n",
        "        jobs_data, resources_data,\n",
        "        energy_weight=best_weights[0],\n",
        "        deadline_weight=best_weights[1],\n",
        "        resilience_weight=best_weights[2]\n",
        "    )\n",
        "\n",
        "    state = final_env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    print(\"\\nFinal Scheduling Simulation:\")\n",
        "    while not done:\n",
        "        with torch.no_grad():\n",
        "            q_values = trained_model(state)\n",
        "            job_id, resource_id, speed_idx = np.unravel_index(\n",
        "                torch.argmax(q_values).item(),\n",
        "                q_values.shape[1:])\n",
        "            speed = [0.8, 1.0, 1.2][speed_idx]\n",
        "            action = (job_id, resource_id, speed)\n",
        "\n",
        "        state, reward, done, info = final_env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        print(f\"Assigned job {job_id} to resource {resource_id} at speed {speed:.1f}. \" +\n",
        "              f\"Energy: {info['energy']:.2f}, Deadline violation: {info['deadline_violation']}, \" +\n",
        "              f\"Reliability: {info['reliability']:.4f}\")\n",
        "\n",
        "    print(\"\\nFinal Results:\")\n",
        "    print(f\"Total reward: {total_reward:.2f}\")\n",
        "    print(f\"Energy consumption: {final_env.energy_consumption:.2f}\")\n",
        "    print(f\"Deadline violations: {final_env.deadline_violations}\")\n",
        "    print(f\"System reliability: {final_env.system_reliability:.4f}\")\n",
        "    print(f\"Jobs completed: {len(final_env.completed_jobs)}/{final_env.n_jobs}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQmJxuadb9Kx",
        "outputId": "b2dd6b64-75a5-46c6-c1ff-ba5b50383d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from ANL-ALCF-DJC-POLARIS_20240101_20241031.csv.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-eb69dabfcf1d>:29: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  'QUEUED_TIMESTAMP': pd.date_range(start='2024-01-01', periods=n_samples, freq='H'),\n",
            "<ipython-input-6-eb69dabfcf1d>:30: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  'START_TIMESTAMP': pd.date_range(start='2024-01-01 01:00:00', periods=n_samples, freq='H'),\n",
            "<ipython-input-6-eb69dabfcf1d>:31: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  'END_TIMESTAMP': pd.date_range(start='2024-01-01 03:00:00', periods=n_samples, freq='H'),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2410.7774200439453, NLL: 0.4981175363063812, KL: 142322.765625\n",
            "Epoch 5, Loss: 1922.4029846191406, NLL: 0.5905016660690308, KL: 116377.6484375\n",
            "Epoch 10, Loss: 1714.0125885009766, NLL: 0.5798592567443848, KL: 104134.4765625\n",
            "Epoch 15, Loss: 1576.244026184082, NLL: 0.5733043551445007, KL: 95897.7578125\n",
            "Epoch 20, Loss: 1473.6988677978516, NLL: 0.5719451308250427, KL: 89718.578125\n",
            "Epoch 25, Loss: 1392.6504974365234, NLL: 0.6264128684997559, KL: 84811.84375\n",
            "Epoch 30, Loss: 1326.134376525879, NLL: 0.6777311563491821, KL: 80778.765625\n",
            "Epoch 35, Loss: 1270.2706756591797, NLL: 0.8118298649787903, KL: 77380.984375\n",
            "Epoch 40, Loss: 1222.102767944336, NLL: 0.4924449622631073, KL: 74469.5859375\n",
            "Epoch 45, Loss: 1180.462989807129, NLL: 0.5420317649841309, KL: 71939.3984375\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}